{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab4e5f3f-542d-403a-8eaf-28ae361f5a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d21b3499-e7ce-45b6-a31d-7c09cab297c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a70cfacb-105a-495c-bcce-279c86c2b0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7782d1fb-510d-4d53-9d09-8af84da1ca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb5151dd-b351-41d4-9bc9-c5c8edba3b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88ed94f4-510a-4f56-b14e-3574538d16bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef0a29a1-4fbf-458c-96b2-ed6e06682bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81712bf2-d0ab-4166-80c5-684dfff55eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "263d72e0-3387-450e-9d26-fe5cd32241fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18ca6e3e-f716-4499-8138-52200d941e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score as r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe527e0d-4687-46d1-90b3-6d1ae0e9d578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "719e1a09-9d54-4569-a3d2-a914010a267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51258a79-2d94-409f-bea4-3efd22df582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f58cb8f2-404f-4f08-a966-fa94a5492b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8768d41-3fcd-4d21-94ef-326b6d3342fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dff843f1-5af8-4f51-b0af-63ddeffebca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6dd81c8d-3d64-4800-91b5-0c00428ab572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python_libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "650ab18e-bab1-4635-90df-acf23be6cb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d59fa44-e44d-43bb-b549-662f787d6cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2191e6e-5d14-4fad-add0-9d25849be6d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6ce764fef0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "528f16ef-ab92-4975-bb5e-45336b8b6d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74cb300d-dbc3-4af8-95d4-451da7cdc3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('car.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c24fb4fe-4662-4685-bd65-bc23a771cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download car.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d0f2dc1-34c5-400a-8ec3-b70550e29772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>model</th>\n",
       "      <th>seller</th>\n",
       "      <th>yearOfRegistration</th>\n",
       "      <th>gearbox</th>\n",
       "      <th>powerps</th>\n",
       "      <th>kilometers</th>\n",
       "      <th>fueltype</th>\n",
       "      <th>brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1993</td>\n",
       "      <td>1</td>\n",
       "      <td>105</td>\n",
       "      <td>150000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>190</td>\n",
       "      <td>125000</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "      <td>2004</td>\n",
       "      <td>2</td>\n",
       "      <td>163</td>\n",
       "      <td>125000</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>150000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  model  seller  yearOfRegistration  gearbox  powerps  kilometers  \\\n",
       "0      0      2       1                1993        1      105      150000   \n",
       "1      1      1       1                2011        1      190      125000   \n",
       "2      2     86       1                2004        2      163      125000   \n",
       "3      3      2       1                2001        1       75      150000   \n",
       "4      4     39       1                2008        1       69       90000   \n",
       "\n",
       "   fueltype  brand  \n",
       "0         1      1  \n",
       "1         2      5  \n",
       "2         2     30  \n",
       "3         1      1  \n",
       "4         2     12  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d818e89-f0ab-49c4-b94b-3002694db4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show head() car.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1bddacb-60f6-4ebf-ad7d-34f33383d1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                 0\n",
       "model                 0\n",
       "seller                0\n",
       "yearOfRegistration    0\n",
       "gearbox               0\n",
       "powerps               0\n",
       "kilometers            0\n",
       "fueltype              0\n",
       "brand                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de2c7f45-0c28-4cb3-af6c-17a5c2aafe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show isna().sum() car.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95686019-805b-44ed-a021-6996e4bc392a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index                 int64\n",
      "model                 int64\n",
      "seller                int64\n",
      "yearOfRegistration    int64\n",
      "gearbox               int64\n",
      "powerps               int64\n",
      "kilometers            int64\n",
      "fueltype              int64\n",
      "brand                 int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4cf64416-f17f-4fc7-bee3-0d1368661c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show dtypes() car.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ff827e8-c0b8-4076-95ea-065b78c03bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 371528 entries, 0 to 371527\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count   Dtype\n",
      "---  ------              --------------   -----\n",
      " 0   index               371528 non-null  int64\n",
      " 1   model               371528 non-null  int64\n",
      " 2   seller              371528 non-null  int64\n",
      " 3   yearOfRegistration  371528 non-null  int64\n",
      " 4   gearbox             371528 non-null  int64\n",
      " 5   powerps             371528 non-null  int64\n",
      " 6   kilometers          371528 non-null  int64\n",
      " 7   fueltype            371528 non-null  int64\n",
      " 8   brand               371528 non-null  int64\n",
      "dtypes: int64(9)\n",
      "memory usage: 25.5 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39651de0-4b91-4e9f-94b1-d1112e690756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show info() car.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fca3de64-7476-46bc-aee0-d60bec295b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(371528, 9)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f64c8f0-f86b-4bdc-8994-3b74a93451a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show shape() car.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d7a1612-876c-4b80-898d-1e93c32e1671",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['brand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c14c739-45cf-4cee-bd28-9f429e32f16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show target(Y) car.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "677c76a0-7e19-4b4a-a5e0-bf54353d8988",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d179cba-d64b-465c-a283-1e21a5e57663",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.iloc[:, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6e75ffe-c17d-45ab-9cb0-50a0bdfd6998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show target(y) car.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0876df36-19a7-4704-a524-2118be13eced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>model</th>\n",
       "      <th>seller</th>\n",
       "      <th>yearOfRegistration</th>\n",
       "      <th>gearbox</th>\n",
       "      <th>powerps</th>\n",
       "      <th>kilometers</th>\n",
       "      <th>fueltype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1993</td>\n",
       "      <td>1</td>\n",
       "      <td>105</td>\n",
       "      <td>150000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>190</td>\n",
       "      <td>125000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "      <td>2004</td>\n",
       "      <td>2</td>\n",
       "      <td>163</td>\n",
       "      <td>125000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>150000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  model  seller  yearOfRegistration  gearbox  powerps  kilometers  \\\n",
       "0      0      2       1                1993        1      105      150000   \n",
       "1      1      1       1                2011        1      190      125000   \n",
       "2      2     86       1                2004        2      163      125000   \n",
       "3      3      2       1                2001        1       75      150000   \n",
       "4      4     39       1                2008        1       69       90000   \n",
       "\n",
       "   fueltype  \n",
       "0         1  \n",
       "1         2  \n",
       "2         2  \n",
       "3         1  \n",
       "4         2  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "18833584-e8d0-4cb2-9ead-cb9341df380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show head() car.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9cdb454-5b7f-49e7-9393-e264e152961d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     1\n",
      "1     5\n",
      "2    30\n",
      "3     1\n",
      "4    12\n",
      "Name: brand, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bd04aa2f-e7a1-4074-a34f-48ab27aed50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show target(y) car.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d9667bf2-abaf-4550-aa14-d95b909d7e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               index          model         seller  yearOfRegistration  \\\n",
      "count  371528.000000  371528.000000  371528.000000       371528.000000   \n",
      "mean   185763.500000      26.496985       0.999001         2004.890961   \n",
      "std    107251.039743      35.612728       0.031585          113.371355   \n",
      "min         0.000000       1.000000       0.000000          400.000000   \n",
      "25%     92881.750000       4.000000       1.000000         1999.000000   \n",
      "50%    185763.500000      12.000000       1.000000         2003.000000   \n",
      "75%    278645.250000      33.000000       1.000000         2008.000000   \n",
      "max    371527.000000     204.000000       1.000000        26700.000000   \n",
      "\n",
      "            gearbox        powerps     kilometers       fueltype  \n",
      "count  371528.00000  371528.000000  371528.000000  371528.000000  \n",
      "mean        1.20838     124.322635  125592.816208       1.514605  \n",
      "std         0.40615      61.272361   40152.086660       0.713596  \n",
      "min         1.00000      10.000000       0.000000       1.000000  \n",
      "25%         1.00000      87.000000  125000.000000       1.000000  \n",
      "50%         1.00000     105.000000  150000.000000       1.000000  \n",
      "75%         1.00000     150.000000  150000.000000       2.000000  \n",
      "max         2.00000    1200.000000  150000.000000       6.000000  \n"
     ]
    }
   ],
   "source": [
    "print(X.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b8c0225e-2160-4147-85d1-32c30f0bd016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show describe() car.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b6f5ad4b-833d-4878-ba4c-c317a2facf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d06f707-84e6-4018-93db-44a02e2c3d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = 0.7, test = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "82680a2f-f20d-492b-9b93-7a08b0d9754f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0cdd2a70-c481-472d-bcc7-c10888d33c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SkLearn.StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c184ce5c-1547-43c0-a591-049943939ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "48d68391-0959-410a-8fc1-03809d5d7e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1aeb1855-e50c-4a8c-8b44-5575db42d62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e3fd7d73-b69a-4cc0-bc3a-cce7142af7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c5ec6772-e32d-48b1-ad20-a9460b804e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e9561360-9511-4b05-a567-c259a042894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SkLearn.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "01747d6c-daa4-402a-afd5-a8a1195226d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = RandomForestRegressor(n_estimators=100, criterion='poisson', max_depth=14, max_features=9, n_jobs = 4, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c6aa6a44-f2e1-4715-8987-9fd7a71b6fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_estimators=100, criterion='poisson', max_depth=14, max_features=9, n_jobs = 4, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d2d64f1d-e648-4717-9db0-b91e8f419035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(criterion=&#x27;poisson&#x27;, max_depth=14, max_features=9,\n",
       "                      n_jobs=4, random_state=40)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(criterion=&#x27;poisson&#x27;, max_depth=14, max_features=9,\n",
       "                      n_jobs=4, random_state=40)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(criterion='poisson', max_depth=14, max_features=9,\n",
       "                      n_jobs=4, random_state=40)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6e7c7c7d-f111-4a58-941a-e528cef8b24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SkLearn.RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "82b6dbb6-d1a3-4d89-9ad2-fc3ce6528553",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a2a24f8a-d602-41fd-a698-dae50cb52690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_predict X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "caf4b346-4d3d-4f9c-9448-4189f218cb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7732585511902744"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3a370e12-a971-4dbf-90de-6cc0cdf986cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SkLearn.R2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "63863300-8852-41cb-8117-636d238c3fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7732585511902744\n"
     ]
    }
   ],
   "source": [
    "print(r2(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a06bd371-b08f-4ae6-b51e-fc3b2cfdd78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SkLearn.R2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d5eb1eb9-eb73-4a1f-b50a-424bf8e6ce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "42bf72b9-28d3-43e1-a5a2-ea5b7710a657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8833380f-cbc2-4a8c-a555-b9366e4f6d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "80a7b546-d38d-46a8-8a7d-29c5c15977d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "82d78afd-49d0-422c-ab40-e75445d5bf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.Tensor(np.array(X_train_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f12d4f9c-6419-41d6-953f-26d353602d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform X_train tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2d53dec7-b9a0-4428-9436-9c6e16031863",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor = torch.Tensor(np.array(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a3352fdb-0910-4b52-a88d-847ff6a793c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform X_test tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cc63b512-ffdf-4da7-a403-496a37541a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tensor = torch.Tensor(np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6c7c74f7-76dc-45b9-bdd2-712c455c2912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform y_train tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2c366905-812e-44fd-aff8-1740204ac863",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_tensor = torch.Tensor(np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b61af4b0-5abb-43ac-97bd-986f141904a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform y_test tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5ad25dce-d5d3-4213-add6-ad88dbf1c261",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data, n_features = X_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bd0493be-a61d-4ef8-8437-6ee7b8f1d08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show shape() n_data, n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "64631360-7263-4c83-b746-cfe9ffb7d0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260069\n"
     ]
    }
   ],
   "source": [
    "print(n_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "78e42772-5d78-42ea-a670-bc696e487b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show shape() n_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "120f45bf-c2f1-4890-8233-65cf3d816282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c471a4d5-2df0-4d3a-93e6-6b34458d8048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show shape() n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0817d6da-2251-4985-8e7e-8c9743d8db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(mean, target):\n",
    "    \"\"\"function loss\"\"\"\n",
    "    return mean(F.l1_loss(input, target, reduction=\"none\") / target) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "aa2445b9-aeb0-4eeb-974b-7a74e1140efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"function loss\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "88cb9a42-fa91-4fab-941b-97d7f0192964",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a27d4277-2599-4c86-af00-e7b6ec651f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5a71a19c-c1d9-4f01-bfaf-d147c5be4d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_func_1 = [loss_func, loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3856b487-ddf6-409b-bee6-a10a429806c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "527a5562-1e9b-4164-8f56-d8c7c19f900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_name = [\"MSE\", \"LOSS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "00719765-d5aa-449d-b381-641ce34bc626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "600a593d-0ebd-4b58-b8e9-1d6d1ceae457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(models, train_data, test_data, models_name):\n",
    "    \"\"\"function print_metrics\"\"\"\n",
    "    results = np.ones(2 * len(models), len(metrics_func_1))\n",
    "    models_name = []\n",
    "    for model in models_name:\n",
    "        models_name.extend([model + \"Train\", model + \"Test\"])\n",
    "    for row, sample in enumerate([train_data, test_data]):\n",
    "        results[row + sample * 2] = evaluate(models, metrics_func_1, sample[0], sample[1])\n",
    "        results = pd.DataFrame(results, columns=metrics_name, index=models_name)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cdc471ab-a7bd-4e1e-9fb9-186b50a0b273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"function print_metrics\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "771529e9-7b28-4e0d-b734-4aa31a7924bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_1 = (X_train_tensor, y_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "532b5a06-2139-4fd6-a501-7b3e92497814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2eaa95e9-a78d-49a8-a576-1e92855cfb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_1 = (X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ebaf5bd5-ca0a-42cf-b1de-c368c8c29d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6dc23241-2f4c-4b59-b2f3-fee8db4c230e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr_sklearn = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2bf50421-4d6a-48d4-b60b-0b95b5865753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SkLearn.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3ff88e07-d0f3-45f5-9b3e-1e832b8070ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr_sklearn.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "def28cc3-77d5-4f47-8561-4cfc0688dae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SkLearn.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fa70e058-fe50-4513-9a9f-8c06ff2291d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = [model_lr_sklearn.predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "71105587-28d7-4063-a1ed-b51a46dc8682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lr_sklearn.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4bde7065-5ea4-4d61-8a56-0b45495428e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_name = [\"MSE\", \"LOSS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "36839c1c-feff-44e1-b82b-2e25144a5ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c2c06952-04e6-41aa-a9ba-b556e4f041d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_name_1 = [\"LOSS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "00e903de-c738-4c97-b849-392b04fca112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "091c87a8-22e8-4601-91eb-dd8b868b977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = nn.Sequential(\n",
    "    nn.Linear(in_features=n_features,\n",
    "    out_features=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cc80278e-4439-4c18-b1fc-646c0c30bad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch model_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "08f4563e-f98f-4ba0-9336-6ab43e23e4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "97a9d682-3e96-4243-9423-ddb17255ff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch model_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7fe742ec-f1c0-43cd-8887-0e0e79885bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_name_1 = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c63c5f34-5c04-49bb-bd54-69b0d46d869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0a3aa294-265b-42a0-9af3-237977c3e672",
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimizer = optim.SGD(params=model_lr.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5d56937c-bf37-4abc-846c-a11217f0db1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.optim model_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "867f69d9-dea4-4f6e-8ca2-3d3956718611",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_LR = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "077d5bd8-a91c-45ad-af95-74c43c93fde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0edaa867-998f-4ee5-be48-cb8fe498a056",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_LR = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fa1cf585-6553-4fc7-bcbb-aba7a15da18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a272f001-f166-4786-8856-f57486177fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [04:37<00:00,  3.61it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm.trange(EPOCHS_LR):\n",
    "    for i in range((n_data - 1) // BATCH_SIZE_LR + 1):\n",
    "        start_i = i * BATCH_SIZE_LR\n",
    "        end_i = start_i + BATCH_SIZE_LR\n",
    "        Xb = X_train_tensor[start_i: end_i]\n",
    "        yb = y_train_tensor[start_i: end_i]\n",
    "        pred = yb\n",
    "        loss_1 = loss_func(pred, yb + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cd3039e3-69ab-4581-a64e-b8805cf54568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in tqdm.trange(EPOCHS_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6beb9f08-92bf-4cf6-b8a2-e6ac7020e2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.52257125  3.47254308  0.03157344 ... -0.31397312  0.60818066\n",
      "  -0.72060886]\n",
      " [-0.02077776 -0.49099807  0.03157344 ... -0.03613951  0.60818066\n",
      "  -0.72060886]\n",
      " [ 1.696281    1.72970938  0.03157344 ... -0.31397312  0.60818066\n",
      "   2.0827602 ]\n",
      " ...\n",
      " [-1.25381022 -0.29422652  0.03157344 ...  0.43781192  0.60818066\n",
      "   0.68107567]\n",
      " [ 0.59811161 -0.18178564  0.03157344 ... -1.14747392  0.60818066\n",
      "  -0.72060886]\n",
      " [-1.56705472 -0.37855719  0.03157344 ... -0.31397312  0.60818066\n",
      "  -0.72060886]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fd32f874-31d8-4789-ba10-307b06635c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5aa88679-b735-4fc7-9bd6-59d636155896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348986     3\n",
      "183474     4\n",
      "367615    15\n",
      "32810      5\n",
      "342339     4\n",
      "          ..\n",
      "349255     5\n",
      "109292     1\n",
      "51241      1\n",
      "249845    13\n",
      "17648      4\n",
      "Name: brand, Length: 111459, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "da38f966-8492-4af2-a572-1c78060cbc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "55e6685c-afd8-4288-bf79-8f52bc03ddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(BATCH_SIZE_LR, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3f29c9a8-52d1-4d8c-9539-24979b6012ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = torch.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "50b199f2-966e-44fd-906f-974e8e9dc78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1.0], [0.0], [0.0], [1.0], [1.0],\n",
    "    [1.0], [0.0], [0.0], [1.0], [1.0],\n",
    "    [1.0], [0.0], [0.0], [1.0], [1.0],\n",
    "    [1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "df43ca69-2d6c-4f43-b439-0c3f56afd8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ad60caaa-4084-4d0c-8e8d-0a9f76978d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  0.9875321388244629\n",
      "epoch:  1  loss:  0.9814753532409668\n",
      "epoch:  2  loss:  0.975476861000061\n",
      "epoch:  3  loss:  0.9695361852645874\n",
      "epoch:  4  loss:  0.9636523723602295\n",
      "epoch:  5  loss:  0.9578251838684082\n",
      "epoch:  6  loss:  0.9520536661148071\n",
      "epoch:  7  loss:  0.9463374614715576\n",
      "epoch:  8  loss:  0.9406757354736328\n",
      "epoch:  9  loss:  0.9350681900978088\n",
      "epoch:  10  loss:  0.9295140504837036\n",
      "epoch:  11  loss:  0.9240126609802246\n",
      "epoch:  12  loss:  0.9185636043548584\n",
      "epoch:  13  loss:  0.913166344165802\n",
      "epoch:  14  loss:  0.9078202247619629\n",
      "epoch:  15  loss:  0.9025247097015381\n",
      "epoch:  16  loss:  0.8972792625427246\n",
      "epoch:  17  loss:  0.8920833468437195\n",
      "epoch:  18  loss:  0.8869363069534302\n",
      "epoch:  19  loss:  0.8818377256393433\n",
      "epoch:  20  loss:  0.8767873048782349\n",
      "epoch:  21  loss:  0.8717840313911438\n",
      "epoch:  22  loss:  0.8668277859687805\n",
      "epoch:  23  loss:  0.8619180917739868\n",
      "epoch:  24  loss:  0.8570541143417358\n",
      "epoch:  25  loss:  0.8522355556488037\n",
      "epoch:  26  loss:  0.847461998462677\n",
      "epoch:  27  loss:  0.8427329063415527\n",
      "epoch:  28  loss:  0.8380476832389832\n",
      "epoch:  29  loss:  0.8334060907363892\n",
      "epoch:  30  loss:  0.8288074731826782\n",
      "epoch:  31  loss:  0.8242513537406921\n",
      "epoch:  32  loss:  0.819737434387207\n",
      "epoch:  33  loss:  0.8152650594711304\n",
      "epoch:  34  loss:  0.8108341097831726\n",
      "epoch:  35  loss:  0.8064438700675964\n",
      "epoch:  36  loss:  0.8020939826965332\n",
      "epoch:  37  loss:  0.7977839708328247\n",
      "epoch:  38  loss:  0.7935135960578918\n",
      "epoch:  39  loss:  0.7892821431159973\n",
      "epoch:  40  loss:  0.7850892543792725\n",
      "epoch:  41  loss:  0.7809349298477173\n",
      "epoch:  42  loss:  0.7768183350563049\n",
      "epoch:  43  loss:  0.7727391123771667\n",
      "epoch:  44  loss:  0.7686969637870789\n",
      "epoch:  45  loss:  0.7646915316581726\n",
      "epoch:  46  loss:  0.7607223987579346\n",
      "epoch:  47  loss:  0.7567890286445618\n",
      "epoch:  48  loss:  0.7528912425041199\n",
      "epoch:  49  loss:  0.7490286231040955\n",
      "epoch:  50  loss:  0.7452008128166199\n",
      "epoch:  51  loss:  0.7414072155952454\n",
      "epoch:  52  loss:  0.7376477718353271\n",
      "epoch:  53  loss:  0.7339220643043518\n",
      "epoch:  54  loss:  0.7302296161651611\n",
      "epoch:  55  loss:  0.7265701293945312\n",
      "epoch:  56  loss:  0.7229433655738831\n",
      "epoch:  57  loss:  0.7193487286567688\n",
      "epoch:  58  loss:  0.7157862186431885\n",
      "epoch:  59  loss:  0.7122554183006287\n",
      "epoch:  60  loss:  0.7087557315826416\n",
      "epoch:  61  loss:  0.705286979675293\n",
      "epoch:  62  loss:  0.7018489837646484\n",
      "epoch:  63  loss:  0.698441207408905\n",
      "epoch:  64  loss:  0.695063591003418\n",
      "epoch:  65  loss:  0.6917155981063843\n",
      "epoch:  66  loss:  0.6883968114852905\n",
      "epoch:  67  loss:  0.6851072907447815\n",
      "epoch:  68  loss:  0.681846559047699\n",
      "epoch:  69  loss:  0.6786143183708191\n",
      "epoch:  70  loss:  0.6754101514816284\n",
      "epoch:  71  loss:  0.6722339987754822\n",
      "epoch:  72  loss:  0.6690854430198669\n",
      "epoch:  73  loss:  0.6659641861915588\n",
      "epoch:  74  loss:  0.662869930267334\n",
      "epoch:  75  loss:  0.6598024964332581\n",
      "epoch:  76  loss:  0.6567615866661072\n",
      "epoch:  77  loss:  0.6537468433380127\n",
      "epoch:  78  loss:  0.6507579684257507\n",
      "epoch:  79  loss:  0.6477949023246765\n",
      "epoch:  80  loss:  0.6448571681976318\n",
      "epoch:  81  loss:  0.6419446468353271\n",
      "epoch:  82  loss:  0.6390570402145386\n",
      "epoch:  83  loss:  0.6361940503120422\n",
      "epoch:  84  loss:  0.6333553791046143\n",
      "epoch:  85  loss:  0.6305409073829651\n",
      "epoch:  86  loss:  0.6277502775192261\n",
      "epoch:  87  loss:  0.6249834895133972\n",
      "epoch:  88  loss:  0.6222399473190308\n",
      "epoch:  89  loss:  0.6195196509361267\n",
      "epoch:  90  loss:  0.6168222427368164\n",
      "epoch:  91  loss:  0.6141476631164551\n",
      "epoch:  92  loss:  0.6114955544471741\n",
      "epoch:  93  loss:  0.6088656187057495\n",
      "epoch:  94  loss:  0.6062576174736023\n",
      "epoch:  95  loss:  0.6036715507507324\n",
      "epoch:  96  loss:  0.6011070013046265\n",
      "epoch:  97  loss:  0.5985639095306396\n",
      "epoch:  98  loss:  0.5960419178009033\n",
      "epoch:  99  loss:  0.5935409069061279\n",
      "epoch:  100  loss:  0.5910605192184448\n",
      "epoch:  101  loss:  0.588600754737854\n",
      "epoch:  102  loss:  0.586161196231842\n",
      "epoch:  103  loss:  0.5837417840957642\n",
      "epoch:  104  loss:  0.5813424587249756\n",
      "epoch:  105  loss:  0.5789626240730286\n",
      "epoch:  106  loss:  0.5766023993492126\n",
      "epoch:  107  loss:  0.574261486530304\n",
      "epoch:  108  loss:  0.5719397068023682\n",
      "epoch:  109  loss:  0.569636881351471\n",
      "epoch:  110  loss:  0.5673527717590332\n",
      "epoch:  111  loss:  0.5650872588157654\n",
      "epoch:  112  loss:  0.5628400444984436\n",
      "epoch:  113  loss:  0.5606110692024231\n",
      "epoch:  114  loss:  0.5584002137184143\n",
      "epoch:  115  loss:  0.5562071204185486\n",
      "epoch:  116  loss:  0.5540316104888916\n",
      "epoch:  117  loss:  0.5518736839294434\n",
      "epoch:  118  loss:  0.54973304271698\n",
      "epoch:  119  loss:  0.5476096272468567\n",
      "epoch:  120  loss:  0.5455030202865601\n",
      "epoch:  121  loss:  0.5434133410453796\n",
      "epoch:  122  loss:  0.5413402915000916\n",
      "epoch:  123  loss:  0.5392837524414062\n",
      "epoch:  124  loss:  0.5372434258460999\n",
      "epoch:  125  loss:  0.5352194309234619\n",
      "epoch:  126  loss:  0.533211350440979\n",
      "epoch:  127  loss:  0.5312191247940063\n",
      "epoch:  128  loss:  0.5292425155639648\n",
      "epoch:  129  loss:  0.5272815227508545\n",
      "epoch:  130  loss:  0.5253360271453857\n",
      "epoch:  131  loss:  0.5234056115150452\n",
      "epoch:  132  loss:  0.5214902758598328\n",
      "epoch:  133  loss:  0.519589900970459\n",
      "epoch:  134  loss:  0.5177044868469238\n",
      "epoch:  135  loss:  0.5158336162567139\n",
      "epoch:  136  loss:  0.5139772295951843\n",
      "epoch:  137  loss:  0.5121352076530457\n",
      "epoch:  138  loss:  0.5103075504302979\n",
      "epoch:  139  loss:  0.5084939002990723\n",
      "epoch:  140  loss:  0.5066943168640137\n",
      "epoch:  141  loss:  0.5049084424972534\n",
      "epoch:  142  loss:  0.5031363368034363\n",
      "epoch:  143  loss:  0.5013777613639832\n",
      "epoch:  144  loss:  0.49963268637657166\n",
      "epoch:  145  loss:  0.49790090322494507\n",
      "epoch:  146  loss:  0.49618226289749146\n",
      "epoch:  147  loss:  0.4944767653942108\n",
      "epoch:  148  loss:  0.49278417229652405\n",
      "epoch:  149  loss:  0.4911043047904968\n",
      "epoch:  150  loss:  0.4894372224807739\n",
      "epoch:  151  loss:  0.48778271675109863\n",
      "epoch:  152  loss:  0.4861406087875366\n",
      "epoch:  153  loss:  0.4845108389854431\n",
      "epoch:  154  loss:  0.4828933775424957\n",
      "epoch:  155  loss:  0.4812878966331482\n",
      "epoch:  156  loss:  0.47969454526901245\n",
      "epoch:  157  loss:  0.47811296582221985\n",
      "epoch:  158  loss:  0.4765431880950928\n",
      "epoch:  159  loss:  0.47498512268066406\n",
      "epoch:  160  loss:  0.473438560962677\n",
      "epoch:  161  loss:  0.47190341353416443\n",
      "epoch:  162  loss:  0.47037962079048157\n",
      "epoch:  163  loss:  0.46886706352233887\n",
      "epoch:  164  loss:  0.4673656225204468\n",
      "epoch:  165  loss:  0.46587514877319336\n",
      "epoch:  166  loss:  0.4643957316875458\n",
      "epoch:  167  loss:  0.462926983833313\n",
      "epoch:  168  loss:  0.46146905422210693\n",
      "epoch:  169  loss:  0.4600217640399933\n",
      "epoch:  170  loss:  0.45858484506607056\n",
      "epoch:  171  loss:  0.4571584463119507\n",
      "epoch:  172  loss:  0.45574232935905457\n",
      "epoch:  173  loss:  0.4543365240097046\n",
      "epoch:  174  loss:  0.45294082164764404\n",
      "epoch:  175  loss:  0.4515551030635834\n",
      "epoch:  176  loss:  0.45017939805984497\n",
      "epoch:  177  loss:  0.4488135874271393\n",
      "epoch:  178  loss:  0.447457492351532\n",
      "epoch:  179  loss:  0.44611114263534546\n",
      "epoch:  180  loss:  0.4447742998600006\n",
      "epoch:  181  loss:  0.4434469938278198\n",
      "epoch:  182  loss:  0.44212913513183594\n",
      "epoch:  183  loss:  0.4408206641674042\n",
      "epoch:  184  loss:  0.43952131271362305\n",
      "epoch:  185  loss:  0.4382312595844269\n",
      "epoch:  186  loss:  0.4369502365589142\n",
      "epoch:  187  loss:  0.4356781840324402\n",
      "epoch:  188  loss:  0.4344150722026825\n",
      "epoch:  189  loss:  0.4331609010696411\n",
      "epoch:  190  loss:  0.43191537261009216\n",
      "epoch:  191  loss:  0.4306785464286804\n",
      "epoch:  192  loss:  0.4294503927230835\n",
      "epoch:  193  loss:  0.4282306730747223\n",
      "epoch:  194  loss:  0.42701947689056396\n",
      "epoch:  195  loss:  0.4258166551589966\n",
      "epoch:  196  loss:  0.4246220588684082\n",
      "epoch:  197  loss:  0.4234358072280884\n",
      "epoch:  198  loss:  0.422257661819458\n",
      "epoch:  199  loss:  0.4210876226425171\n",
      "epoch:  200  loss:  0.4199255704879761\n",
      "epoch:  201  loss:  0.41877150535583496\n",
      "epoch:  202  loss:  0.4176252782344818\n",
      "epoch:  203  loss:  0.4164868891239166\n",
      "epoch:  204  loss:  0.41535624861717224\n",
      "epoch:  205  loss:  0.41423317790031433\n",
      "epoch:  206  loss:  0.41311773657798767\n",
      "epoch:  207  loss:  0.4120098650455475\n",
      "epoch:  208  loss:  0.4109094440937042\n",
      "epoch:  209  loss:  0.4098165035247803\n",
      "epoch:  210  loss:  0.40873080492019653\n",
      "epoch:  211  loss:  0.40765246748924255\n",
      "epoch:  212  loss:  0.406581312417984\n",
      "epoch:  213  loss:  0.40551722049713135\n",
      "epoch:  214  loss:  0.4044603407382965\n",
      "epoch:  215  loss:  0.4034104645252228\n",
      "epoch:  216  loss:  0.402367502450943\n",
      "epoch:  217  loss:  0.40133148431777954\n",
      "epoch:  218  loss:  0.40030229091644287\n",
      "epoch:  219  loss:  0.399279922246933\n",
      "epoch:  220  loss:  0.39826422929763794\n",
      "epoch:  221  loss:  0.3972552716732025\n",
      "epoch:  222  loss:  0.3962528705596924\n",
      "epoch:  223  loss:  0.3952571153640747\n",
      "epoch:  224  loss:  0.3942677676677704\n",
      "epoch:  225  loss:  0.3932848870754242\n",
      "epoch:  226  loss:  0.39230847358703613\n",
      "epoch:  227  loss:  0.3913383185863495\n",
      "epoch:  228  loss:  0.39037448167800903\n",
      "epoch:  229  loss:  0.3894168436527252\n",
      "epoch:  230  loss:  0.38846543431282043\n",
      "epoch:  231  loss:  0.3875201642513275\n",
      "epoch:  232  loss:  0.3865809142589569\n",
      "epoch:  233  loss:  0.3856477439403534\n",
      "epoch:  234  loss:  0.38472050428390503\n",
      "epoch:  235  loss:  0.3837992548942566\n",
      "epoch:  236  loss:  0.38288384675979614\n",
      "epoch:  237  loss:  0.3819742798805237\n",
      "epoch:  238  loss:  0.38107046484947205\n",
      "epoch:  239  loss:  0.38017240166664124\n",
      "epoch:  240  loss:  0.3792800009250641\n",
      "epoch:  241  loss:  0.3783932626247406\n",
      "epoch:  242  loss:  0.3775120973587036\n",
      "epoch:  243  loss:  0.3766365051269531\n",
      "epoch:  244  loss:  0.3757663667201996\n",
      "epoch:  245  loss:  0.374901682138443\n",
      "epoch:  246  loss:  0.3740423917770386\n",
      "epoch:  247  loss:  0.3731886148452759\n",
      "epoch:  248  loss:  0.37234002351760864\n",
      "epoch:  249  loss:  0.37149670720100403\n",
      "epoch:  250  loss:  0.37065863609313965\n",
      "epoch:  251  loss:  0.3698257505893707\n",
      "epoch:  252  loss:  0.36899805068969727\n",
      "epoch:  253  loss:  0.3681754171848297\n",
      "epoch:  254  loss:  0.36735787987709045\n",
      "epoch:  255  loss:  0.3665454089641571\n",
      "epoch:  256  loss:  0.36573782563209534\n",
      "epoch:  257  loss:  0.3649352490901947\n",
      "epoch:  258  loss:  0.36413756012916565\n",
      "epoch:  259  loss:  0.36334478855133057\n",
      "epoch:  260  loss:  0.3625567555427551\n",
      "epoch:  261  loss:  0.3617735505104065\n",
      "epoch:  262  loss:  0.3609951436519623\n",
      "epoch:  263  loss:  0.36022135615348816\n",
      "epoch:  264  loss:  0.3594523072242737\n",
      "epoch:  265  loss:  0.3586878776550293\n",
      "epoch:  266  loss:  0.3579280376434326\n",
      "epoch:  267  loss:  0.35717272758483887\n",
      "epoch:  268  loss:  0.3564220070838928\n",
      "epoch:  269  loss:  0.3556758165359497\n",
      "epoch:  270  loss:  0.35493406653404236\n",
      "epoch:  271  loss:  0.35419660806655884\n",
      "epoch:  272  loss:  0.35346370935440063\n",
      "epoch:  273  loss:  0.3527350425720215\n",
      "epoch:  274  loss:  0.35201069712638855\n",
      "epoch:  275  loss:  0.3512907028198242\n",
      "epoch:  276  loss:  0.35057491064071655\n",
      "epoch:  277  loss:  0.3498634099960327\n",
      "epoch:  278  loss:  0.349155992269516\n",
      "epoch:  279  loss:  0.3484528362751007\n",
      "epoch:  280  loss:  0.34775370359420776\n",
      "epoch:  281  loss:  0.3470586836338043\n",
      "epoch:  282  loss:  0.3463677763938904\n",
      "epoch:  283  loss:  0.345680832862854\n",
      "epoch:  284  loss:  0.34499791264533997\n",
      "epoch:  285  loss:  0.3443189859390259\n",
      "epoch:  286  loss:  0.3436439335346222\n",
      "epoch:  287  loss:  0.3429728150367737\n",
      "epoch:  288  loss:  0.34230560064315796\n",
      "epoch:  289  loss:  0.34164220094680786\n",
      "epoch:  290  loss:  0.3409826457500458\n",
      "epoch:  291  loss:  0.34032684564590454\n",
      "epoch:  292  loss:  0.33967480063438416\n",
      "epoch:  293  loss:  0.339026540517807\n",
      "epoch:  294  loss:  0.33838188648223877\n",
      "epoch:  295  loss:  0.337740957736969\n",
      "epoch:  296  loss:  0.3371036946773529\n",
      "epoch:  297  loss:  0.3364700376987457\n",
      "epoch:  298  loss:  0.33583998680114746\n",
      "epoch:  299  loss:  0.33521348237991333\n",
      "epoch:  300  loss:  0.33459049463272095\n",
      "epoch:  301  loss:  0.3339710533618927\n",
      "epoch:  302  loss:  0.3333550989627838\n",
      "epoch:  303  loss:  0.3327426016330719\n",
      "epoch:  304  loss:  0.33213356137275696\n",
      "epoch:  305  loss:  0.33152785897254944\n",
      "epoch:  306  loss:  0.3309255540370941\n",
      "epoch:  307  loss:  0.3303266763687134\n",
      "epoch:  308  loss:  0.3297311067581177\n",
      "epoch:  309  loss:  0.32913878560066223\n",
      "epoch:  310  loss:  0.3285498321056366\n",
      "epoch:  311  loss:  0.32796409726142883\n",
      "epoch:  312  loss:  0.32738161087036133\n",
      "epoch:  313  loss:  0.3268023431301117\n",
      "epoch:  314  loss:  0.32622623443603516\n",
      "epoch:  315  loss:  0.3256533443927765\n",
      "epoch:  316  loss:  0.32508358359336853\n",
      "epoch:  317  loss:  0.3245169520378113\n",
      "epoch:  318  loss:  0.3239533603191376\n",
      "epoch:  319  loss:  0.32339292764663696\n",
      "epoch:  320  loss:  0.3228355050086975\n",
      "epoch:  321  loss:  0.3222810924053192\n",
      "epoch:  322  loss:  0.32172974944114685\n",
      "epoch:  323  loss:  0.3211813271045685\n",
      "epoch:  324  loss:  0.32063594460487366\n",
      "epoch:  325  loss:  0.32009345293045044\n",
      "epoch:  326  loss:  0.319553941488266\n",
      "epoch:  327  loss:  0.31901729106903076\n",
      "epoch:  328  loss:  0.31848353147506714\n",
      "epoch:  329  loss:  0.3179526627063751\n",
      "epoch:  330  loss:  0.31742459535598755\n",
      "epoch:  331  loss:  0.3168993592262268\n",
      "epoch:  332  loss:  0.31637701392173767\n",
      "epoch:  333  loss:  0.3158573508262634\n",
      "epoch:  334  loss:  0.315340518951416\n",
      "epoch:  335  loss:  0.3148263692855835\n",
      "epoch:  336  loss:  0.31431496143341064\n",
      "epoch:  337  loss:  0.31380629539489746\n",
      "epoch:  338  loss:  0.3133002817630768\n",
      "epoch:  339  loss:  0.3127969801425934\n",
      "epoch:  340  loss:  0.3122963011264801\n",
      "epoch:  341  loss:  0.3117982745170593\n",
      "epoch:  342  loss:  0.3113028109073639\n",
      "epoch:  343  loss:  0.31080999970436096\n",
      "epoch:  344  loss:  0.3103197515010834\n",
      "epoch:  345  loss:  0.30983206629753113\n",
      "epoch:  346  loss:  0.30934691429138184\n",
      "epoch:  347  loss:  0.3088642656803131\n",
      "epoch:  348  loss:  0.3083841800689697\n",
      "epoch:  349  loss:  0.3079065680503845\n",
      "epoch:  350  loss:  0.3074313998222351\n",
      "epoch:  351  loss:  0.30695870518684387\n",
      "epoch:  352  loss:  0.3064884543418884\n",
      "epoch:  353  loss:  0.30602067708969116\n",
      "epoch:  354  loss:  0.30555522441864014\n",
      "epoch:  355  loss:  0.3050921857357025\n",
      "epoch:  356  loss:  0.3046315610408783\n",
      "epoch:  357  loss:  0.3041732907295227\n",
      "epoch:  358  loss:  0.30371731519699097\n",
      "epoch:  359  loss:  0.30326366424560547\n",
      "epoch:  360  loss:  0.302812397480011\n",
      "epoch:  361  loss:  0.3023633658885956\n",
      "epoch:  362  loss:  0.3019166588783264\n",
      "epoch:  363  loss:  0.30147218704223633\n",
      "epoch:  364  loss:  0.3010299801826477\n",
      "epoch:  365  loss:  0.30058997869491577\n",
      "epoch:  366  loss:  0.3001522719860077\n",
      "epoch:  367  loss:  0.2997167408466339\n",
      "epoch:  368  loss:  0.29928338527679443\n",
      "epoch:  369  loss:  0.29885220527648926\n",
      "epoch:  370  loss:  0.2984232008457184\n",
      "epoch:  371  loss:  0.29799631237983704\n",
      "epoch:  372  loss:  0.29757159948349\n",
      "epoch:  373  loss:  0.29714900255203247\n",
      "epoch:  374  loss:  0.29672855138778687\n",
      "epoch:  375  loss:  0.2963101267814636\n",
      "epoch:  376  loss:  0.2958937883377075\n",
      "epoch:  377  loss:  0.29547953605651855\n",
      "epoch:  378  loss:  0.29506736993789673\n",
      "epoch:  379  loss:  0.2946572005748749\n",
      "epoch:  380  loss:  0.2942490875720978\n",
      "epoch:  381  loss:  0.29384297132492065\n",
      "epoch:  382  loss:  0.2934388518333435\n",
      "epoch:  383  loss:  0.29303669929504395\n",
      "epoch:  384  loss:  0.29263657331466675\n",
      "epoch:  385  loss:  0.29223835468292236\n",
      "epoch:  386  loss:  0.29184213280677795\n",
      "epoch:  387  loss:  0.29144784808158875\n",
      "epoch:  388  loss:  0.29105544090270996\n",
      "epoch:  389  loss:  0.29066500067710876\n",
      "epoch:  390  loss:  0.290276437997818\n",
      "epoch:  391  loss:  0.28988978266716003\n",
      "epoch:  392  loss:  0.2895049452781677\n",
      "epoch:  393  loss:  0.2891220450401306\n",
      "epoch:  394  loss:  0.28874093294143677\n",
      "epoch:  395  loss:  0.28836172819137573\n",
      "epoch:  396  loss:  0.28798431158065796\n",
      "epoch:  397  loss:  0.28760871291160583\n",
      "epoch:  398  loss:  0.28723496198654175\n",
      "epoch:  399  loss:  0.2868629992008209\n",
      "epoch:  400  loss:  0.2864927649497986\n",
      "epoch:  401  loss:  0.2861243486404419\n",
      "epoch:  402  loss:  0.2857576906681061\n",
      "epoch:  403  loss:  0.2853928208351135\n",
      "epoch:  404  loss:  0.2850296199321747\n",
      "epoch:  405  loss:  0.2846681475639343\n",
      "epoch:  406  loss:  0.28430843353271484\n",
      "epoch:  407  loss:  0.28395044803619385\n",
      "epoch:  408  loss:  0.28359416127204895\n",
      "epoch:  409  loss:  0.28323954343795776\n",
      "epoch:  410  loss:  0.2828866243362427\n",
      "epoch:  411  loss:  0.2825353443622589\n",
      "epoch:  412  loss:  0.28218570351600647\n",
      "epoch:  413  loss:  0.28183770179748535\n",
      "epoch:  414  loss:  0.28149136900901794\n",
      "epoch:  415  loss:  0.28114667534828186\n",
      "epoch:  416  loss:  0.2808035612106323\n",
      "epoch:  417  loss:  0.2804620862007141\n",
      "epoch:  418  loss:  0.28012222051620483\n",
      "epoch:  419  loss:  0.2797839343547821\n",
      "epoch:  420  loss:  0.27944719791412354\n",
      "epoch:  421  loss:  0.2791120409965515\n",
      "epoch:  422  loss:  0.27877846360206604\n",
      "epoch:  423  loss:  0.2784464359283447\n",
      "epoch:  424  loss:  0.2781159579753876\n",
      "epoch:  425  loss:  0.2777869701385498\n",
      "epoch:  426  loss:  0.2774595618247986\n",
      "epoch:  427  loss:  0.277133584022522\n",
      "epoch:  428  loss:  0.2768091559410095\n",
      "epoch:  429  loss:  0.27648624777793884\n",
      "epoch:  430  loss:  0.2761647701263428\n",
      "epoch:  431  loss:  0.27584484219551086\n",
      "epoch:  432  loss:  0.27552637457847595\n",
      "epoch:  433  loss:  0.27520930767059326\n",
      "epoch:  434  loss:  0.27489373087882996\n",
      "epoch:  435  loss:  0.27457958459854126\n",
      "epoch:  436  loss:  0.27426689863204956\n",
      "epoch:  437  loss:  0.27395564317703247\n",
      "epoch:  438  loss:  0.2736457586288452\n",
      "epoch:  439  loss:  0.27333733439445496\n",
      "epoch:  440  loss:  0.27303025126457214\n",
      "epoch:  441  loss:  0.27272459864616394\n",
      "epoch:  442  loss:  0.27242034673690796\n",
      "epoch:  443  loss:  0.2721174657344818\n",
      "epoch:  444  loss:  0.2718159556388855\n",
      "epoch:  445  loss:  0.27151578664779663\n",
      "epoch:  446  loss:  0.2712169885635376\n",
      "epoch:  447  loss:  0.2709195017814636\n",
      "epoch:  448  loss:  0.27062341570854187\n",
      "epoch:  449  loss:  0.2703286409378052\n",
      "epoch:  450  loss:  0.27003514766693115\n",
      "epoch:  451  loss:  0.26974302530288696\n",
      "epoch:  452  loss:  0.26945218443870544\n",
      "epoch:  453  loss:  0.2691626250743866\n",
      "epoch:  454  loss:  0.2688743770122528\n",
      "epoch:  455  loss:  0.2685874104499817\n",
      "epoch:  456  loss:  0.26830172538757324\n",
      "epoch:  457  loss:  0.26801735162734985\n",
      "epoch:  458  loss:  0.267734169960022\n",
      "epoch:  459  loss:  0.26745226979255676\n",
      "epoch:  460  loss:  0.26717162132263184\n",
      "epoch:  461  loss:  0.2668922543525696\n",
      "epoch:  462  loss:  0.26661407947540283\n",
      "epoch:  463  loss:  0.266337126493454\n",
      "epoch:  464  loss:  0.2660614252090454\n",
      "epoch:  465  loss:  0.26578691601753235\n",
      "epoch:  466  loss:  0.2655136287212372\n",
      "epoch:  467  loss:  0.2652415633201599\n",
      "epoch:  468  loss:  0.26497066020965576\n",
      "epoch:  469  loss:  0.2647009491920471\n",
      "epoch:  470  loss:  0.264432430267334\n",
      "epoch:  471  loss:  0.2641650438308716\n",
      "epoch:  472  loss:  0.2638988792896271\n",
      "epoch:  473  loss:  0.2636338472366333\n",
      "epoch:  474  loss:  0.26337000727653503\n",
      "epoch:  475  loss:  0.2631072998046875\n",
      "epoch:  476  loss:  0.2628456950187683\n",
      "epoch:  477  loss:  0.26258525252342224\n",
      "epoch:  478  loss:  0.2623259723186493\n",
      "epoch:  479  loss:  0.2620677649974823\n",
      "epoch:  480  loss:  0.2618107199668884\n",
      "epoch:  481  loss:  0.2615547776222229\n",
      "epoch:  482  loss:  0.2612999379634857\n",
      "epoch:  483  loss:  0.2610461711883545\n",
      "epoch:  484  loss:  0.2607935070991516\n",
      "epoch:  485  loss:  0.26054197549819946\n",
      "epoch:  486  loss:  0.2602914869785309\n",
      "epoch:  487  loss:  0.26004207134246826\n",
      "epoch:  488  loss:  0.2597937285900116\n",
      "epoch:  489  loss:  0.2595464885234833\n",
      "epoch:  490  loss:  0.25930023193359375\n",
      "epoch:  491  loss:  0.25905507802963257\n",
      "epoch:  492  loss:  0.25881096720695496\n",
      "epoch:  493  loss:  0.2585678696632385\n",
      "epoch:  494  loss:  0.25832587480545044\n",
      "epoch:  495  loss:  0.25808486342430115\n",
      "epoch:  496  loss:  0.2578448951244354\n",
      "epoch:  497  loss:  0.2576059401035309\n",
      "epoch:  498  loss:  0.2573679983615875\n",
      "epoch:  499  loss:  0.25713104009628296\n",
      "epoch:  500  loss:  0.2568950951099396\n",
      "epoch:  501  loss:  0.2566601634025574\n",
      "epoch:  502  loss:  0.25642621517181396\n",
      "epoch:  503  loss:  0.25619328022003174\n",
      "epoch:  504  loss:  0.2559612989425659\n",
      "epoch:  505  loss:  0.2557303011417389\n",
      "epoch:  506  loss:  0.25550028681755066\n",
      "epoch:  507  loss:  0.25527122616767883\n",
      "epoch:  508  loss:  0.255043089389801\n",
      "epoch:  509  loss:  0.2548159956932068\n",
      "epoch:  510  loss:  0.2545897960662842\n",
      "epoch:  511  loss:  0.254364550113678\n",
      "epoch:  512  loss:  0.2541402578353882\n",
      "epoch:  513  loss:  0.2539169192314148\n",
      "epoch:  514  loss:  0.25369447469711304\n",
      "epoch:  515  loss:  0.2534729540348053\n",
      "epoch:  516  loss:  0.2532523572444916\n",
      "epoch:  517  loss:  0.2530326843261719\n",
      "epoch:  518  loss:  0.2528139352798462\n",
      "epoch:  519  loss:  0.25259608030319214\n",
      "epoch:  520  loss:  0.2523791790008545\n",
      "epoch:  521  loss:  0.2521631121635437\n",
      "epoch:  522  loss:  0.25194793939590454\n",
      "epoch:  523  loss:  0.2517336905002594\n",
      "epoch:  524  loss:  0.2515203654766083\n",
      "epoch:  525  loss:  0.25130781531333923\n",
      "epoch:  526  loss:  0.2510961890220642\n",
      "epoch:  527  loss:  0.2508854568004608\n",
      "epoch:  528  loss:  0.2506755590438843\n",
      "epoch:  529  loss:  0.25046658515930176\n",
      "epoch:  530  loss:  0.2502583861351013\n",
      "epoch:  531  loss:  0.2500510811805725\n",
      "epoch:  532  loss:  0.24984461069107056\n",
      "epoch:  533  loss:  0.24963901937007904\n",
      "epoch:  534  loss:  0.249434232711792\n",
      "epoch:  535  loss:  0.249230295419693\n",
      "epoch:  536  loss:  0.24902717769145966\n",
      "epoch:  537  loss:  0.24882489442825317\n",
      "epoch:  538  loss:  0.24862347543239594\n",
      "epoch:  539  loss:  0.24842283129692078\n",
      "epoch:  540  loss:  0.24822299182415009\n",
      "epoch:  541  loss:  0.24802400171756744\n",
      "epoch:  542  loss:  0.24782578647136688\n",
      "epoch:  543  loss:  0.24762839078903198\n",
      "epoch:  544  loss:  0.24743178486824036\n",
      "epoch:  545  loss:  0.2472359985113144\n",
      "epoch:  546  loss:  0.24704095721244812\n",
      "epoch:  547  loss:  0.2468467354774475\n",
      "epoch:  548  loss:  0.24665328860282898\n",
      "epoch:  549  loss:  0.24646063148975372\n",
      "epoch:  550  loss:  0.24626874923706055\n",
      "epoch:  551  loss:  0.24607762694358826\n",
      "epoch:  552  loss:  0.24588729441165924\n",
      "epoch:  553  loss:  0.24569770693778992\n",
      "epoch:  554  loss:  0.24550887942314148\n",
      "epoch:  555  loss:  0.24532079696655273\n",
      "epoch:  556  loss:  0.24513348937034607\n",
      "epoch:  557  loss:  0.2449469119310379\n",
      "epoch:  558  loss:  0.24476110935211182\n",
      "epoch:  559  loss:  0.24457603693008423\n",
      "epoch:  560  loss:  0.24439167976379395\n",
      "epoch:  561  loss:  0.24420805275440216\n",
      "epoch:  562  loss:  0.24402517080307007\n",
      "epoch:  563  loss:  0.24384303390979767\n",
      "epoch:  564  loss:  0.24366161227226257\n",
      "epoch:  565  loss:  0.2434808909893036\n",
      "epoch:  566  loss:  0.2433008849620819\n",
      "epoch:  567  loss:  0.24312162399291992\n",
      "epoch:  568  loss:  0.24294304847717285\n",
      "epoch:  569  loss:  0.2427651733160019\n",
      "epoch:  570  loss:  0.24258804321289062\n",
      "epoch:  571  loss:  0.2424115538597107\n",
      "epoch:  572  loss:  0.24223580956459045\n",
      "epoch:  573  loss:  0.24206072092056274\n",
      "epoch:  574  loss:  0.24188634753227234\n",
      "epoch:  575  loss:  0.24171262979507446\n",
      "epoch:  576  loss:  0.24153965711593628\n",
      "epoch:  577  loss:  0.24136728048324585\n",
      "epoch:  578  loss:  0.24119561910629272\n",
      "epoch:  579  loss:  0.24102462828159332\n",
      "epoch:  580  loss:  0.24085430800914764\n",
      "epoch:  581  loss:  0.2406846582889557\n",
      "epoch:  582  loss:  0.24051564931869507\n",
      "epoch:  583  loss:  0.24034732580184937\n",
      "epoch:  584  loss:  0.2401796579360962\n",
      "epoch:  585  loss:  0.24001264572143555\n",
      "epoch:  586  loss:  0.23984624445438385\n",
      "epoch:  587  loss:  0.23968054354190826\n",
      "epoch:  588  loss:  0.239515483379364\n",
      "epoch:  589  loss:  0.23935100436210632\n",
      "epoch:  590  loss:  0.23918721079826355\n",
      "epoch:  591  loss:  0.2390240728855133\n",
      "epoch:  592  loss:  0.23886153101921082\n",
      "epoch:  593  loss:  0.23869960010051727\n",
      "epoch:  594  loss:  0.23853830993175507\n",
      "epoch:  595  loss:  0.2383776754140854\n",
      "epoch:  596  loss:  0.23821760714054108\n",
      "epoch:  597  loss:  0.23805822432041168\n",
      "epoch:  598  loss:  0.23789940774440765\n",
      "epoch:  599  loss:  0.23774118721485138\n",
      "epoch:  600  loss:  0.23758363723754883\n",
      "epoch:  601  loss:  0.23742660880088806\n",
      "epoch:  602  loss:  0.2372702658176422\n",
      "epoch:  603  loss:  0.23711447417736053\n",
      "epoch:  604  loss:  0.2369593232870102\n",
      "epoch:  605  loss:  0.23680472373962402\n",
      "epoch:  606  loss:  0.2366507202386856\n",
      "epoch:  607  loss:  0.23649731278419495\n",
      "epoch:  608  loss:  0.23634450137615204\n",
      "epoch:  609  loss:  0.2361922562122345\n",
      "epoch:  610  loss:  0.23604059219360352\n",
      "epoch:  611  loss:  0.2358895242214203\n",
      "epoch:  612  loss:  0.23573902249336243\n",
      "epoch:  613  loss:  0.23558910191059113\n",
      "epoch:  614  loss:  0.235439732670784\n",
      "epoch:  615  loss:  0.23529094457626343\n",
      "epoch:  616  loss:  0.23514269292354584\n",
      "epoch:  617  loss:  0.234995037317276\n",
      "epoch:  618  loss:  0.23484793305397034\n",
      "epoch:  619  loss:  0.23470138013362885\n",
      "epoch:  620  loss:  0.23455537855625153\n",
      "epoch:  621  loss:  0.23440992832183838\n",
      "epoch:  622  loss:  0.2342650145292282\n",
      "epoch:  623  loss:  0.2341206818819046\n",
      "epoch:  624  loss:  0.23397688567638397\n",
      "epoch:  625  loss:  0.23383362591266632\n",
      "epoch:  626  loss:  0.23369088768959045\n",
      "epoch:  627  loss:  0.23354868590831757\n",
      "epoch:  628  loss:  0.23340705037117004\n",
      "epoch:  629  loss:  0.2332659214735031\n",
      "epoch:  630  loss:  0.23312532901763916\n",
      "epoch:  631  loss:  0.232985258102417\n",
      "epoch:  632  loss:  0.2328457236289978\n",
      "epoch:  633  loss:  0.232706680893898\n",
      "epoch:  634  loss:  0.2325681895017624\n",
      "epoch:  635  loss:  0.23243021965026855\n",
      "epoch:  636  loss:  0.23229272663593292\n",
      "epoch:  637  loss:  0.23215577006340027\n",
      "epoch:  638  loss:  0.2320193350315094\n",
      "epoch:  639  loss:  0.23188339173793793\n",
      "epoch:  640  loss:  0.23174795508384705\n",
      "epoch:  641  loss:  0.23161303997039795\n",
      "epoch:  642  loss:  0.23147860169410706\n",
      "epoch:  643  loss:  0.23134468495845795\n",
      "epoch:  644  loss:  0.23121124505996704\n",
      "epoch:  645  loss:  0.23107829689979553\n",
      "epoch:  646  loss:  0.2309458702802658\n",
      "epoch:  647  loss:  0.2308138906955719\n",
      "epoch:  648  loss:  0.23068241775035858\n",
      "epoch:  649  loss:  0.23055145144462585\n",
      "epoch:  650  loss:  0.23042094707489014\n",
      "epoch:  651  loss:  0.23029091954231262\n",
      "epoch:  652  loss:  0.2301614135503769\n",
      "epoch:  653  loss:  0.23003233969211578\n",
      "epoch:  654  loss:  0.22990374267101288\n",
      "epoch:  655  loss:  0.22977563738822937\n",
      "epoch:  656  loss:  0.22964796423912048\n",
      "epoch:  657  loss:  0.229520782828331\n",
      "epoch:  658  loss:  0.2293940931558609\n",
      "epoch:  659  loss:  0.22926786541938782\n",
      "epoch:  660  loss:  0.22914206981658936\n",
      "epoch:  661  loss:  0.2290167659521103\n",
      "epoch:  662  loss:  0.22889189422130585\n",
      "epoch:  663  loss:  0.22876745462417603\n",
      "epoch:  664  loss:  0.2286435216665268\n",
      "epoch:  665  loss:  0.22852002084255219\n",
      "epoch:  666  loss:  0.228396937251091\n",
      "epoch:  667  loss:  0.22827434539794922\n",
      "epoch:  668  loss:  0.22815220057964325\n",
      "epoch:  669  loss:  0.2280304878950119\n",
      "epoch:  670  loss:  0.22790922224521637\n",
      "epoch:  671  loss:  0.22778837382793427\n",
      "epoch:  672  loss:  0.22766798734664917\n",
      "epoch:  673  loss:  0.2275480180978775\n",
      "epoch:  674  loss:  0.22742851078510284\n",
      "epoch:  675  loss:  0.22730940580368042\n",
      "epoch:  676  loss:  0.22719073295593262\n",
      "epoch:  677  loss:  0.22707252204418182\n",
      "epoch:  678  loss:  0.22695471346378326\n",
      "epoch:  679  loss:  0.22683733701705933\n",
      "epoch:  680  loss:  0.22672037780284882\n",
      "epoch:  681  loss:  0.22660383582115173\n",
      "epoch:  682  loss:  0.22648774087429047\n",
      "epoch:  683  loss:  0.22637201845645905\n",
      "epoch:  684  loss:  0.22625674307346344\n",
      "epoch:  685  loss:  0.22614187002182007\n",
      "epoch:  686  loss:  0.22602742910385132\n",
      "epoch:  687  loss:  0.2259133756160736\n",
      "epoch:  688  loss:  0.22579973936080933\n",
      "epoch:  689  loss:  0.22568649053573608\n",
      "epoch:  690  loss:  0.22557367384433746\n",
      "epoch:  691  loss:  0.22546124458312988\n",
      "epoch:  692  loss:  0.22534923255443573\n",
      "epoch:  693  loss:  0.225237637758255\n",
      "epoch:  694  loss:  0.22512637078762054\n",
      "epoch:  695  loss:  0.2250155508518219\n",
      "epoch:  696  loss:  0.2249051332473755\n",
      "epoch:  697  loss:  0.22479510307312012\n",
      "epoch:  698  loss:  0.2246854305267334\n",
      "epoch:  699  loss:  0.2245761752128601\n",
      "epoch:  700  loss:  0.22446732223033905\n",
      "epoch:  701  loss:  0.22435882687568665\n",
      "epoch:  702  loss:  0.22425071895122528\n",
      "epoch:  703  loss:  0.22414299845695496\n",
      "epoch:  704  loss:  0.22403568029403687\n",
      "epoch:  705  loss:  0.22392871975898743\n",
      "epoch:  706  loss:  0.22382214665412903\n",
      "epoch:  707  loss:  0.22371596097946167\n",
      "epoch:  708  loss:  0.22361013293266296\n",
      "epoch:  709  loss:  0.22350464761257172\n",
      "epoch:  710  loss:  0.2233995646238327\n",
      "epoch:  711  loss:  0.22329485416412354\n",
      "epoch:  712  loss:  0.2231905311346054\n",
      "epoch:  713  loss:  0.22308655083179474\n",
      "epoch:  714  loss:  0.22298292815685272\n",
      "epoch:  715  loss:  0.22287967801094055\n",
      "epoch:  716  loss:  0.22277680039405823\n",
      "epoch:  717  loss:  0.22267426550388336\n",
      "epoch:  718  loss:  0.22257211804389954\n",
      "epoch:  719  loss:  0.22247028350830078\n",
      "epoch:  720  loss:  0.22236885130405426\n",
      "epoch:  721  loss:  0.2222677618265152\n",
      "epoch:  722  loss:  0.2221670150756836\n",
      "epoch:  723  loss:  0.22206662595272064\n",
      "epoch:  724  loss:  0.22196659445762634\n",
      "epoch:  725  loss:  0.2218668907880783\n",
      "epoch:  726  loss:  0.22176754474639893\n",
      "epoch:  727  loss:  0.2216685563325882\n",
      "epoch:  728  loss:  0.22156986594200134\n",
      "epoch:  729  loss:  0.22147154808044434\n",
      "epoch:  730  loss:  0.2213735729455948\n",
      "epoch:  731  loss:  0.2212759405374527\n",
      "epoch:  732  loss:  0.22117866575717926\n",
      "epoch:  733  loss:  0.2210816890001297\n",
      "epoch:  734  loss:  0.22098508477210999\n",
      "epoch:  735  loss:  0.22088879346847534\n",
      "epoch:  736  loss:  0.22079284489154816\n",
      "epoch:  737  loss:  0.22069720923900604\n",
      "epoch:  738  loss:  0.22060193121433258\n",
      "epoch:  739  loss:  0.22050698101520538\n",
      "epoch:  740  loss:  0.22041234374046326\n",
      "epoch:  741  loss:  0.2203180193901062\n",
      "epoch:  742  loss:  0.2202240377664566\n",
      "epoch:  743  loss:  0.22013036906719208\n",
      "epoch:  744  loss:  0.220037043094635\n",
      "epoch:  745  loss:  0.21994401514530182\n",
      "epoch:  746  loss:  0.2198513150215149\n",
      "epoch:  747  loss:  0.21975892782211304\n",
      "epoch:  748  loss:  0.21966685354709625\n",
      "epoch:  749  loss:  0.21957510709762573\n",
      "epoch:  750  loss:  0.21948367357254028\n",
      "epoch:  751  loss:  0.21939252316951752\n",
      "epoch:  752  loss:  0.2193017452955246\n",
      "epoch:  753  loss:  0.21921122074127197\n",
      "epoch:  754  loss:  0.2191210240125656\n",
      "epoch:  755  loss:  0.2190311700105667\n",
      "epoch:  756  loss:  0.2189415693283081\n",
      "epoch:  757  loss:  0.21885228157043457\n",
      "epoch:  758  loss:  0.2187633067369461\n",
      "epoch:  759  loss:  0.2186746448278427\n",
      "epoch:  760  loss:  0.218586266040802\n",
      "epoch:  761  loss:  0.21849820017814636\n",
      "epoch:  762  loss:  0.2184104323387146\n",
      "epoch:  763  loss:  0.2183229625225067\n",
      "epoch:  764  loss:  0.2182358056306839\n",
      "epoch:  765  loss:  0.21814890205860138\n",
      "epoch:  766  loss:  0.21806231141090393\n",
      "epoch:  767  loss:  0.21797603368759155\n",
      "epoch:  768  loss:  0.21788999438285828\n",
      "epoch:  769  loss:  0.21780429780483246\n",
      "epoch:  770  loss:  0.21771889925003052\n",
      "epoch:  771  loss:  0.21763373911380768\n",
      "epoch:  772  loss:  0.2175488919019699\n",
      "epoch:  773  loss:  0.21746432781219482\n",
      "epoch:  774  loss:  0.21738003194332123\n",
      "epoch:  775  loss:  0.2172960489988327\n",
      "epoch:  776  loss:  0.21721233427524567\n",
      "epoch:  777  loss:  0.2171289026737213\n",
      "epoch:  778  loss:  0.21704573929309845\n",
      "epoch:  779  loss:  0.21696287393569946\n",
      "epoch:  780  loss:  0.21688027679920197\n",
      "epoch:  781  loss:  0.21679794788360596\n",
      "epoch:  782  loss:  0.21671590209007263\n",
      "epoch:  783  loss:  0.216634139418602\n",
      "epoch:  784  loss:  0.21655263006687164\n",
      "epoch:  785  loss:  0.21647140383720398\n",
      "epoch:  786  loss:  0.216390460729599\n",
      "epoch:  787  loss:  0.21630975604057312\n",
      "epoch:  788  loss:  0.21622934937477112\n",
      "epoch:  789  loss:  0.2161492109298706\n",
      "epoch:  790  loss:  0.2160693109035492\n",
      "epoch:  791  loss:  0.21598970890045166\n",
      "epoch:  792  loss:  0.21591034531593323\n",
      "epoch:  793  loss:  0.2158312350511551\n",
      "epoch:  794  loss:  0.21575240790843964\n",
      "epoch:  795  loss:  0.21567386388778687\n",
      "epoch:  796  loss:  0.215595543384552\n",
      "epoch:  797  loss:  0.21551747620105743\n",
      "epoch:  798  loss:  0.21543970704078674\n",
      "epoch:  799  loss:  0.21536217629909515\n",
      "epoch:  800  loss:  0.21528489887714386\n",
      "epoch:  801  loss:  0.21520788967609406\n",
      "epoch:  802  loss:  0.21513111889362335\n",
      "epoch:  803  loss:  0.21505458652973175\n",
      "epoch:  804  loss:  0.21497830748558044\n",
      "epoch:  805  loss:  0.21490229666233063\n",
      "epoch:  806  loss:  0.2148265242576599\n",
      "epoch:  807  loss:  0.2147510051727295\n",
      "epoch:  808  loss:  0.21467573940753937\n",
      "epoch:  809  loss:  0.21460072696208954\n",
      "epoch:  810  loss:  0.21452593803405762\n",
      "epoch:  811  loss:  0.21445143222808838\n",
      "epoch:  812  loss:  0.21437714993953705\n",
      "epoch:  813  loss:  0.21430307626724243\n",
      "epoch:  814  loss:  0.2142293006181717\n",
      "epoch:  815  loss:  0.21415573358535767\n",
      "epoch:  816  loss:  0.21408240497112274\n",
      "epoch:  817  loss:  0.2140093296766281\n",
      "epoch:  818  loss:  0.2139364778995514\n",
      "epoch:  819  loss:  0.21386386454105377\n",
      "epoch:  820  loss:  0.21379151940345764\n",
      "epoch:  821  loss:  0.21371938288211823\n",
      "epoch:  822  loss:  0.21364746987819672\n",
      "epoch:  823  loss:  0.2135757952928543\n",
      "epoch:  824  loss:  0.21350440382957458\n",
      "epoch:  825  loss:  0.213433176279068\n",
      "epoch:  826  loss:  0.2133622169494629\n",
      "epoch:  827  loss:  0.2132914513349533\n",
      "epoch:  828  loss:  0.21322095394134521\n",
      "epoch:  829  loss:  0.21315068006515503\n",
      "epoch:  830  loss:  0.21308061480522156\n",
      "epoch:  831  loss:  0.2130107879638672\n",
      "epoch:  832  loss:  0.21294116973876953\n",
      "epoch:  833  loss:  0.21287177503108978\n",
      "epoch:  834  loss:  0.21280261874198914\n",
      "epoch:  835  loss:  0.2127336859703064\n",
      "epoch:  836  loss:  0.21266496181488037\n",
      "epoch:  837  loss:  0.21259650588035583\n",
      "epoch:  838  loss:  0.21252821385860443\n",
      "epoch:  839  loss:  0.21246016025543213\n",
      "epoch:  840  loss:  0.21239233016967773\n",
      "epoch:  841  loss:  0.21232469379901886\n",
      "epoch:  842  loss:  0.2122572958469391\n",
      "epoch:  843  loss:  0.21219010651111603\n",
      "epoch:  844  loss:  0.21212315559387207\n",
      "epoch:  845  loss:  0.21205639839172363\n",
      "epoch:  846  loss:  0.2119898498058319\n",
      "epoch:  847  loss:  0.2119235098361969\n",
      "epoch:  848  loss:  0.2118573933839798\n",
      "epoch:  849  loss:  0.2117914855480194\n",
      "epoch:  850  loss:  0.21172580122947693\n",
      "epoch:  851  loss:  0.21166029572486877\n",
      "epoch:  852  loss:  0.21159499883651733\n",
      "epoch:  853  loss:  0.211529940366745\n",
      "epoch:  854  loss:  0.21146506071090698\n",
      "epoch:  855  loss:  0.21140043437480927\n",
      "epoch:  856  loss:  0.21133597195148468\n",
      "epoch:  857  loss:  0.21127170324325562\n",
      "epoch:  858  loss:  0.21120765805244446\n",
      "epoch:  859  loss:  0.21114380657672882\n",
      "epoch:  860  loss:  0.2110801786184311\n",
      "epoch:  861  loss:  0.2110167294740677\n",
      "epoch:  862  loss:  0.2109534740447998\n",
      "epoch:  863  loss:  0.21089044213294983\n",
      "epoch:  864  loss:  0.21082758903503418\n",
      "epoch:  865  loss:  0.21076494455337524\n",
      "epoch:  866  loss:  0.21070249378681183\n",
      "epoch:  867  loss:  0.21064025163650513\n",
      "epoch:  868  loss:  0.21057818830013275\n",
      "epoch:  869  loss:  0.2105163335800171\n",
      "epoch:  870  loss:  0.21045467257499695\n",
      "epoch:  871  loss:  0.21039319038391113\n",
      "epoch:  872  loss:  0.21033190190792084\n",
      "epoch:  873  loss:  0.21027082204818726\n",
      "epoch:  874  loss:  0.210209921002388\n",
      "epoch:  875  loss:  0.21014921367168427\n",
      "epoch:  876  loss:  0.21008868515491486\n",
      "epoch:  877  loss:  0.21002836525440216\n",
      "epoch:  878  loss:  0.20996823906898499\n",
      "epoch:  879  loss:  0.20990827679634094\n",
      "epoch:  880  loss:  0.2098485231399536\n",
      "epoch:  881  loss:  0.20978893339633942\n",
      "epoch:  882  loss:  0.20972953736782074\n",
      "epoch:  883  loss:  0.20967033505439758\n",
      "epoch:  884  loss:  0.20961131155490875\n",
      "epoch:  885  loss:  0.20955245196819305\n",
      "epoch:  886  loss:  0.20949380099773407\n",
      "epoch:  887  loss:  0.2094353288412094\n",
      "epoch:  888  loss:  0.20937703549861908\n",
      "epoch:  889  loss:  0.20931893587112427\n",
      "epoch:  890  loss:  0.2092609852552414\n",
      "epoch:  891  loss:  0.20920324325561523\n",
      "epoch:  892  loss:  0.209145650267601\n",
      "epoch:  893  loss:  0.2090882509946823\n",
      "epoch:  894  loss:  0.20903103053569794\n",
      "epoch:  895  loss:  0.2089739888906479\n",
      "epoch:  896  loss:  0.20891711115837097\n",
      "epoch:  897  loss:  0.20886042714118958\n",
      "epoch:  898  loss:  0.2088039070367813\n",
      "epoch:  899  loss:  0.20874758064746857\n",
      "epoch:  900  loss:  0.20869137346744537\n",
      "epoch:  901  loss:  0.2086353898048401\n",
      "epoch:  902  loss:  0.20857955515384674\n",
      "epoch:  903  loss:  0.2085239142179489\n",
      "epoch:  904  loss:  0.20846839249134064\n",
      "epoch:  905  loss:  0.20841309428215027\n",
      "epoch:  906  loss:  0.20835795998573303\n",
      "epoch:  907  loss:  0.20830297470092773\n",
      "epoch:  908  loss:  0.20824816823005676\n",
      "epoch:  909  loss:  0.20819351077079773\n",
      "epoch:  910  loss:  0.20813903212547302\n",
      "epoch:  911  loss:  0.20808474719524384\n",
      "epoch:  912  loss:  0.2080306112766266\n",
      "epoch:  913  loss:  0.20797662436962128\n",
      "epoch:  914  loss:  0.2079228013753891\n",
      "epoch:  915  loss:  0.20786917209625244\n",
      "epoch:  916  loss:  0.20781566202640533\n",
      "epoch:  917  loss:  0.20776231586933136\n",
      "epoch:  918  loss:  0.2077091634273529\n",
      "epoch:  919  loss:  0.20765617489814758\n",
      "epoch:  920  loss:  0.2076033055782318\n",
      "epoch:  921  loss:  0.20755064487457275\n",
      "epoch:  922  loss:  0.20749811828136444\n",
      "epoch:  923  loss:  0.20744577050209045\n",
      "epoch:  924  loss:  0.2073935568332672\n",
      "epoch:  925  loss:  0.2073415219783783\n",
      "epoch:  926  loss:  0.20728962123394012\n",
      "epoch:  927  loss:  0.2072378695011139\n",
      "epoch:  928  loss:  0.2071862816810608\n",
      "epoch:  929  loss:  0.20713485777378082\n",
      "epoch:  930  loss:  0.2070835828781128\n",
      "epoch:  931  loss:  0.2070324867963791\n",
      "epoch:  932  loss:  0.20698150992393494\n",
      "epoch:  933  loss:  0.2069307118654251\n",
      "epoch:  934  loss:  0.20688007771968842\n",
      "epoch:  935  loss:  0.20682953298091888\n",
      "epoch:  936  loss:  0.20677916705608368\n",
      "epoch:  937  loss:  0.206728994846344\n",
      "epoch:  938  loss:  0.20667894184589386\n",
      "epoch:  939  loss:  0.20662903785705566\n",
      "epoch:  940  loss:  0.2065792977809906\n",
      "epoch:  941  loss:  0.20652969181537628\n",
      "epoch:  942  loss:  0.2064802199602127\n",
      "epoch:  943  loss:  0.20643091201782227\n",
      "epoch:  944  loss:  0.20638175308704376\n",
      "epoch:  945  loss:  0.2063327431678772\n",
      "epoch:  946  loss:  0.20628386735916138\n",
      "epoch:  947  loss:  0.2062351405620575\n",
      "epoch:  948  loss:  0.20618656277656555\n",
      "epoch:  949  loss:  0.20613813400268555\n",
      "epoch:  950  loss:  0.2060898244380951\n",
      "epoch:  951  loss:  0.20604167878627777\n",
      "epoch:  952  loss:  0.2059936672449112\n",
      "epoch:  953  loss:  0.20594580471515656\n",
      "epoch:  954  loss:  0.20589809119701385\n",
      "epoch:  955  loss:  0.2058505266904831\n",
      "epoch:  956  loss:  0.2058030664920807\n",
      "epoch:  957  loss:  0.20575575530529022\n",
      "epoch:  958  loss:  0.2057085931301117\n",
      "epoch:  959  loss:  0.2056615948677063\n",
      "epoch:  960  loss:  0.20561470091342926\n",
      "epoch:  961  loss:  0.20556795597076416\n",
      "epoch:  962  loss:  0.2055213451385498\n",
      "epoch:  963  loss:  0.2054748684167862\n",
      "epoch:  964  loss:  0.20542852580547333\n",
      "epoch:  965  loss:  0.2053823322057724\n",
      "epoch:  966  loss:  0.20533624291419983\n",
      "epoch:  967  loss:  0.20529033243656158\n",
      "epoch:  968  loss:  0.2052445262670517\n",
      "epoch:  969  loss:  0.20519886910915375\n",
      "epoch:  970  loss:  0.20515334606170654\n",
      "epoch:  971  loss:  0.2051079422235489\n",
      "epoch:  972  loss:  0.20506270229816437\n",
      "epoch:  973  loss:  0.2050175815820694\n",
      "epoch:  974  loss:  0.2049725502729416\n",
      "epoch:  975  loss:  0.2049277126789093\n",
      "epoch:  976  loss:  0.20488297939300537\n",
      "epoch:  977  loss:  0.2048383504152298\n",
      "epoch:  978  loss:  0.20479385554790497\n",
      "epoch:  979  loss:  0.20474952459335327\n",
      "epoch:  980  loss:  0.20470529794692993\n",
      "epoch:  981  loss:  0.20466122031211853\n",
      "epoch:  982  loss:  0.20461724698543549\n",
      "epoch:  983  loss:  0.20457340776920319\n",
      "epoch:  984  loss:  0.20452970266342163\n",
      "epoch:  985  loss:  0.20448613166809082\n",
      "epoch:  986  loss:  0.20444266498088837\n",
      "epoch:  987  loss:  0.20439933240413666\n",
      "epoch:  988  loss:  0.2043561190366745\n",
      "epoch:  989  loss:  0.20431305468082428\n",
      "epoch:  990  loss:  0.20427006483078003\n",
      "epoch:  991  loss:  0.2042272686958313\n",
      "epoch:  992  loss:  0.20418453216552734\n",
      "epoch:  993  loss:  0.20414195954799652\n",
      "epoch:  994  loss:  0.20409949123859406\n",
      "epoch:  995  loss:  0.20405711233615875\n",
      "epoch:  996  loss:  0.20401494204998016\n",
      "epoch:  997  loss:  0.20397281646728516\n",
      "epoch:  998  loss:  0.2039308398962021\n",
      "epoch:  999  loss:  0.20388898253440857\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    y_pred = model_lr(X)\n",
    "    loss = metrics_name_1(y_pred, y)\n",
    "    print('epoch: ', epoch,' loss: ', loss.item())\n",
    "    Optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    Optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "15c8bb89-5da3-4493-a745-f4389043ae5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "dcdbe744-082a-4406-8262-5bf5fb698f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_lr, loss_fn, optimizer):\n",
    "    \"\"\"function train\"\"\"\n",
    "    size = len(X_train_tensor)\n",
    "    model_lr.train()\n",
    "    for batch_size_lr in range(1000):\n",
    "        y_pred_1 = model_lr(X)\n",
    "        loss = loss_fn(y_pred_1, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if BATCH_SIZE_LR % 16 == 0:\n",
    "            loss_2 = loss.item(), BATCH_SIZE_LR * len(X)\n",
    "            print(f'loss: {loss_2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9e5d4634-5f63-4c55-8ecc-59a4a16bd62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"function train\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "cf0be15c-5252-4d4a-8d1e-1d6a27ebe014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model_lr, loss_fn):\n",
    "    \"\"\"function test\"\"\"\n",
    "    size = len(X_train_tensor)\n",
    "    num_batches = size\n",
    "    model_lr.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_size_lr in range(1000):\n",
    "            y_pred_2 = model_lr(X)\n",
    "            test_loss += loss_fn(y_pred_2, y).item()\n",
    "            test_loss /= num_batches\n",
    "            print(f'Test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "717e97ac-e390-4746-9d42-2beed8bdac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"function test\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0ab630bf-4e10-4e57-af2c-49d6b7089ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "424a9f99-3f18-4841-8f71-3e09140ee545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a56b4104-087b-47e1-b345-9b0891281eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(EPOCHS_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3f196883-d2e3-4285-9903-3f88a52476e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# google-chrome-stable 119 -error memory chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "96cde03c-6e77-47b8-8160-7de7eea64641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results(for epoch in range(EPOCHS_LR)): to see python_script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "113c2cbc-a10d-43d3-abc6-373738a1e4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5847]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model_lr(X_test_tensor[-1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "32076c4d-9bbc-4503-bbdb-0b4b644e5914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_tensor[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f163ff1d-57e1-4517-a9e3-ab2fb2a5f0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_lr, \"lr.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "37fb6813-f5f1-4167-8975-33229190aec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a168c047-6d96-4183-905a-daac2e0be802",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr_1 = nn.Sequential(\n",
    "    nn.Linear(in_features=n_features, out_features=16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=16, out_features=32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=32, out_features=32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=32, out_features=32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=32, out_features=32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=32, out_features=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ee921a40-2c4a-43dd-8989-42c6a4123719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch model_lr_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "db6dbe59-72ec-4913-99a7-c4a73b7623a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=8, out_features=16, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (9): ReLU()\n",
      "  (10): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_lr_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "073c22a0-2d69-443a-9d53-2ba57ba1b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch model_lr_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5a6608c2-dd79-4bdb-93e7-7e652905f8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_name_2 = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4c01c8a3-8784-4ee0-b942-00ef984e7f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "80b02a6a-cbf1-4414-8558-040a61e93b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimizer = optim.SGD(params=model_lr_1.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "9f7c9f53-083d-4d6e-9ef5-81241fa142c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.optim model_lr_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "36285075-9f15-4bdc-b5a6-6779236869e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_LR_1 = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f791ec7f-c315-48ec-87cb-b5a069c68d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "05fa59dc-a30d-407a-8477-754daf933a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_LR_1 = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9d24f6fd-2653-4341-ad1b-b2450eaa7323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2ce03e44-6aed-4a01-91cd-692bca3c92a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [04:30<00:00,  3.70it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm.trange(EPOCHS_LR_1):\n",
    "    for i in range((n_data - 1) // BATCH_SIZE_LR_1 + 1):\n",
    "        start_i = i * BATCH_SIZE_LR_1\n",
    "        end_i = start_i + BATCH_SIZE_LR_1\n",
    "        Xb = X_train_tensor[start_i: end_i]\n",
    "        yb = y_train_tensor[start_i: end_i]\n",
    "        pred_2 = yb\n",
    "        loss = loss_func(pred_2, yb + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5e643229-e682-49d0-9511-503ea533c1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in tqdm.trange(EPOCHS_LR_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "19c384c0-d52e-4a9d-8caf-47d32927e596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.52257125  3.47254308  0.03157344 ... -0.31397312  0.60818066\n",
      "  -0.72060886]\n",
      " [-0.02077776 -0.49099807  0.03157344 ... -0.03613951  0.60818066\n",
      "  -0.72060886]\n",
      " [ 1.696281    1.72970938  0.03157344 ... -0.31397312  0.60818066\n",
      "   2.0827602 ]\n",
      " ...\n",
      " [-1.25381022 -0.29422652  0.03157344 ...  0.43781192  0.60818066\n",
      "   0.68107567]\n",
      " [ 0.59811161 -0.18178564  0.03157344 ... -1.14747392  0.60818066\n",
      "  -0.72060886]\n",
      " [-1.56705472 -0.37855719  0.03157344 ... -0.31397312  0.60818066\n",
      "  -0.72060886]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "78fe55ef-b3c6-4091-8e2b-0df222e1a083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "3fbd83ec-b539-4031-94de-3b7a4fe57e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348986     3\n",
      "183474     4\n",
      "367615    15\n",
      "32810      5\n",
      "342339     4\n",
      "          ..\n",
      "349255     5\n",
      "109292     1\n",
      "51241      1\n",
      "249845    13\n",
      "17648      4\n",
      "Name: brand, Length: 111459, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "98c4c0e0-127e-4b35-9ecb-f120a490a583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "0438a26e-9fcc-439d-ba7f-06d5fb349147",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(BATCH_SIZE_LR_1, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "dead3292-56a6-48cf-b718-df1dc22a0267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d7546631-7055-4761-bbae-265001c38c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1.0], [0.0], [0.0], [1.0], [1.0],\n",
    "    [1.0], [0.0], [0.0], [1.0], [1.0],\n",
    "    [1.0], [0.0], [0.0], [1.0], [1.0],\n",
    "    [1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "cb0881cd-27b3-4a08-a297-c807ff4a8eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f852b13c-0d71-4acc-8481-e33cd9ccb239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  0.5355349779129028\n",
      "epoch:  1  loss:  0.535366415977478\n",
      "epoch:  2  loss:  0.5351979732513428\n",
      "epoch:  3  loss:  0.5350297093391418\n",
      "epoch:  4  loss:  0.5348614454269409\n",
      "epoch:  5  loss:  0.5346933603286743\n",
      "epoch:  6  loss:  0.5345253348350525\n",
      "epoch:  7  loss:  0.5343574285507202\n",
      "epoch:  8  loss:  0.5341895222663879\n",
      "epoch:  9  loss:  0.5340218544006348\n",
      "epoch:  10  loss:  0.5338542461395264\n",
      "epoch:  11  loss:  0.5336866974830627\n",
      "epoch:  12  loss:  0.5335192084312439\n",
      "epoch:  13  loss:  0.5333518981933594\n",
      "epoch:  14  loss:  0.5331846475601196\n",
      "epoch:  15  loss:  0.5330175161361694\n",
      "epoch:  16  loss:  0.5328503847122192\n",
      "epoch:  17  loss:  0.5326834917068481\n",
      "epoch:  18  loss:  0.532516598701477\n",
      "epoch:  19  loss:  0.5323498845100403\n",
      "epoch:  20  loss:  0.5321832895278931\n",
      "epoch:  21  loss:  0.5320166349411011\n",
      "epoch:  22  loss:  0.5318502187728882\n",
      "epoch:  23  loss:  0.5316838026046753\n",
      "epoch:  24  loss:  0.5315175652503967\n",
      "epoch:  25  loss:  0.5313513278961182\n",
      "epoch:  26  loss:  0.5311853289604187\n",
      "epoch:  27  loss:  0.5310193300247192\n",
      "epoch:  28  loss:  0.5308535099029541\n",
      "epoch:  29  loss:  0.5306876301765442\n",
      "epoch:  30  loss:  0.5305220484733582\n",
      "epoch:  31  loss:  0.5303563475608826\n",
      "epoch:  32  loss:  0.5301909446716309\n",
      "epoch:  33  loss:  0.5300254821777344\n",
      "epoch:  34  loss:  0.5298601388931274\n",
      "epoch:  35  loss:  0.5296949744224548\n",
      "epoch:  36  loss:  0.5295297503471375\n",
      "epoch:  37  loss:  0.5293646454811096\n",
      "epoch:  38  loss:  0.5291996598243713\n",
      "epoch:  39  loss:  0.5290346741676331\n",
      "epoch:  40  loss:  0.5288699269294739\n",
      "epoch:  41  loss:  0.5287052392959595\n",
      "epoch:  42  loss:  0.5285405516624451\n",
      "epoch:  43  loss:  0.528376042842865\n",
      "epoch:  44  loss:  0.5282115936279297\n",
      "epoch:  45  loss:  0.5280472040176392\n",
      "epoch:  46  loss:  0.5278830528259277\n",
      "epoch:  47  loss:  0.5277189016342163\n",
      "epoch:  48  loss:  0.5275548696517944\n",
      "epoch:  49  loss:  0.5273909568786621\n",
      "epoch:  50  loss:  0.5272271037101746\n",
      "epoch:  51  loss:  0.5270633101463318\n",
      "epoch:  52  loss:  0.5268996357917786\n",
      "epoch:  53  loss:  0.5267361402511597\n",
      "epoch:  54  loss:  0.5265726447105408\n",
      "epoch:  55  loss:  0.5264092683792114\n",
      "epoch:  56  loss:  0.5262459516525269\n",
      "epoch:  57  loss:  0.5260828137397766\n",
      "epoch:  58  loss:  0.5259197354316711\n",
      "epoch:  59  loss:  0.5257567167282104\n",
      "epoch:  60  loss:  0.5255938172340393\n",
      "epoch:  61  loss:  0.5254309773445129\n",
      "epoch:  62  loss:  0.5252682566642761\n",
      "epoch:  63  loss:  0.5251057147979736\n",
      "epoch:  64  loss:  0.5249431133270264\n",
      "epoch:  65  loss:  0.5247807502746582\n",
      "epoch:  66  loss:  0.52461838722229\n",
      "epoch:  67  loss:  0.5244561433792114\n",
      "epoch:  68  loss:  0.5242939591407776\n",
      "epoch:  69  loss:  0.5241318941116333\n",
      "epoch:  70  loss:  0.5239699482917786\n",
      "epoch:  71  loss:  0.5238081216812134\n",
      "epoch:  72  loss:  0.5236462950706482\n",
      "epoch:  73  loss:  0.5234846472740173\n",
      "epoch:  74  loss:  0.5233230590820312\n",
      "epoch:  75  loss:  0.5231615304946899\n",
      "epoch:  76  loss:  0.523000180721283\n",
      "epoch:  77  loss:  0.522838830947876\n",
      "epoch:  78  loss:  0.5226776599884033\n",
      "epoch:  79  loss:  0.5225165486335754\n",
      "epoch:  80  loss:  0.5223554968833923\n",
      "epoch:  81  loss:  0.522194504737854\n",
      "epoch:  82  loss:  0.52203369140625\n",
      "epoch:  83  loss:  0.5218729972839355\n",
      "epoch:  84  loss:  0.5217123031616211\n",
      "epoch:  85  loss:  0.521551787853241\n",
      "epoch:  86  loss:  0.5213913321495056\n",
      "epoch:  87  loss:  0.521230936050415\n",
      "epoch:  88  loss:  0.5210705995559692\n",
      "epoch:  89  loss:  0.5209105014801025\n",
      "epoch:  90  loss:  0.5207504034042358\n",
      "epoch:  91  loss:  0.5205904245376587\n",
      "epoch:  92  loss:  0.5204305052757263\n",
      "epoch:  93  loss:  0.5202707052230835\n",
      "epoch:  94  loss:  0.5201109647750854\n",
      "epoch:  95  loss:  0.5199514031410217\n",
      "epoch:  96  loss:  0.5197919011116028\n",
      "epoch:  97  loss:  0.5196324586868286\n",
      "epoch:  98  loss:  0.5194730758666992\n",
      "epoch:  99  loss:  0.5193138718605042\n",
      "epoch:  100  loss:  0.5191546678543091\n",
      "epoch:  101  loss:  0.5189956426620483\n",
      "epoch:  102  loss:  0.5188367366790771\n",
      "epoch:  103  loss:  0.5186777710914612\n",
      "epoch:  104  loss:  0.5185190439224243\n",
      "epoch:  105  loss:  0.5183603167533875\n",
      "epoch:  106  loss:  0.5182017683982849\n",
      "epoch:  107  loss:  0.5180432200431824\n",
      "epoch:  108  loss:  0.5178848505020142\n",
      "epoch:  109  loss:  0.517726480960846\n",
      "epoch:  110  loss:  0.5175682902336121\n",
      "epoch:  111  loss:  0.517410159111023\n",
      "epoch:  112  loss:  0.5172520875930786\n",
      "epoch:  113  loss:  0.517094075679779\n",
      "epoch:  114  loss:  0.5169362425804138\n",
      "epoch:  115  loss:  0.5167784690856934\n",
      "epoch:  116  loss:  0.5166207551956177\n",
      "epoch:  117  loss:  0.5164631605148315\n",
      "epoch:  118  loss:  0.516305685043335\n",
      "epoch:  119  loss:  0.5161482691764832\n",
      "epoch:  120  loss:  0.5159909725189209\n",
      "epoch:  121  loss:  0.5158337354660034\n",
      "epoch:  122  loss:  0.5156766176223755\n",
      "epoch:  123  loss:  0.5155195593833923\n",
      "epoch:  124  loss:  0.515362560749054\n",
      "epoch:  125  loss:  0.5152057409286499\n",
      "epoch:  126  loss:  0.5150489211082458\n",
      "epoch:  127  loss:  0.5148922801017761\n",
      "epoch:  128  loss:  0.5147356390953064\n",
      "epoch:  129  loss:  0.514579176902771\n",
      "epoch:  130  loss:  0.5144227147102356\n",
      "epoch:  131  loss:  0.5142663717269897\n",
      "epoch:  132  loss:  0.5141100287437439\n",
      "epoch:  133  loss:  0.5139537453651428\n",
      "epoch:  134  loss:  0.5137975811958313\n",
      "epoch:  135  loss:  0.5136415958404541\n",
      "epoch:  136  loss:  0.5134856104850769\n",
      "epoch:  137  loss:  0.5133297443389893\n",
      "epoch:  138  loss:  0.5131739377975464\n",
      "epoch:  139  loss:  0.5130181908607483\n",
      "epoch:  140  loss:  0.5128626227378845\n",
      "epoch:  141  loss:  0.5127071142196655\n",
      "epoch:  142  loss:  0.5125516653060913\n",
      "epoch:  143  loss:  0.5123963356018066\n",
      "epoch:  144  loss:  0.5122411251068115\n",
      "epoch:  145  loss:  0.5120859146118164\n",
      "epoch:  146  loss:  0.5119308829307556\n",
      "epoch:  147  loss:  0.5117759108543396\n",
      "epoch:  148  loss:  0.5116209983825684\n",
      "epoch:  149  loss:  0.5114662051200867\n",
      "epoch:  150  loss:  0.5113114714622498\n",
      "epoch:  151  loss:  0.5111569166183472\n",
      "epoch:  152  loss:  0.5110023617744446\n",
      "epoch:  153  loss:  0.5108479261398315\n",
      "epoch:  154  loss:  0.5106936097145081\n",
      "epoch:  155  loss:  0.5105392932891846\n",
      "epoch:  156  loss:  0.5103851556777954\n",
      "epoch:  157  loss:  0.510231077671051\n",
      "epoch:  158  loss:  0.5100771188735962\n",
      "epoch:  159  loss:  0.5099231600761414\n",
      "epoch:  160  loss:  0.5097693800926208\n",
      "epoch:  161  loss:  0.5096156597137451\n",
      "epoch:  162  loss:  0.5094620585441589\n",
      "epoch:  163  loss:  0.5093085169792175\n",
      "epoch:  164  loss:  0.5091550350189209\n",
      "epoch:  165  loss:  0.5090016722679138\n",
      "epoch:  166  loss:  0.5088484287261963\n",
      "epoch:  167  loss:  0.5086951851844788\n",
      "epoch:  168  loss:  0.5085421204566956\n",
      "epoch:  169  loss:  0.5083891153335571\n",
      "epoch:  170  loss:  0.5082361698150635\n",
      "epoch:  171  loss:  0.5080833435058594\n",
      "epoch:  172  loss:  0.5079305171966553\n",
      "epoch:  173  loss:  0.5077778100967407\n",
      "epoch:  174  loss:  0.5076251029968262\n",
      "epoch:  175  loss:  0.5074725151062012\n",
      "epoch:  176  loss:  0.5073200464248657\n",
      "epoch:  177  loss:  0.507167637348175\n",
      "epoch:  178  loss:  0.5070152878761292\n",
      "epoch:  179  loss:  0.5068631172180176\n",
      "epoch:  180  loss:  0.506710946559906\n",
      "epoch:  181  loss:  0.5065588355064392\n",
      "epoch:  182  loss:  0.5064069032669067\n",
      "epoch:  183  loss:  0.5062550902366638\n",
      "epoch:  184  loss:  0.506103515625\n",
      "epoch:  185  loss:  0.505952000617981\n",
      "epoch:  186  loss:  0.5058005452156067\n",
      "epoch:  187  loss:  0.5056491494178772\n",
      "epoch:  188  loss:  0.505497932434082\n",
      "epoch:  189  loss:  0.5053468346595764\n",
      "epoch:  190  loss:  0.5051957368850708\n",
      "epoch:  191  loss:  0.50504469871521\n",
      "epoch:  192  loss:  0.5048938393592834\n",
      "epoch:  193  loss:  0.5047430396080017\n",
      "epoch:  194  loss:  0.50459223985672\n",
      "epoch:  195  loss:  0.5044416189193726\n",
      "epoch:  196  loss:  0.5042910575866699\n",
      "epoch:  197  loss:  0.5041406154632568\n",
      "epoch:  198  loss:  0.5039901733398438\n",
      "epoch:  199  loss:  0.503839910030365\n",
      "epoch:  200  loss:  0.503689706325531\n",
      "epoch:  201  loss:  0.5035395622253418\n",
      "epoch:  202  loss:  0.5033895373344421\n",
      "epoch:  203  loss:  0.503239631652832\n",
      "epoch:  204  loss:  0.5030897259712219\n",
      "epoch:  205  loss:  0.5029399394989014\n",
      "epoch:  206  loss:  0.5027902722358704\n",
      "epoch:  207  loss:  0.5026406645774841\n",
      "epoch:  208  loss:  0.5024911761283875\n",
      "epoch:  209  loss:  0.5023417472839355\n",
      "epoch:  210  loss:  0.5021923780441284\n",
      "epoch:  211  loss:  0.5020431280136108\n",
      "epoch:  212  loss:  0.501893937587738\n",
      "epoch:  213  loss:  0.50174480676651\n",
      "epoch:  214  loss:  0.5015957951545715\n",
      "epoch:  215  loss:  0.5014469623565674\n",
      "epoch:  216  loss:  0.5012981295585632\n",
      "epoch:  217  loss:  0.5011493563652039\n",
      "epoch:  218  loss:  0.5010006427764893\n",
      "epoch:  219  loss:  0.500852108001709\n",
      "epoch:  220  loss:  0.5007036924362183\n",
      "epoch:  221  loss:  0.5005552172660828\n",
      "epoch:  222  loss:  0.5004069209098816\n",
      "epoch:  223  loss:  0.50025874376297\n",
      "epoch:  224  loss:  0.5001106262207031\n",
      "epoch:  225  loss:  0.4999624788761139\n",
      "epoch:  226  loss:  0.49981456995010376\n",
      "epoch:  227  loss:  0.49966666102409363\n",
      "epoch:  228  loss:  0.49951884150505066\n",
      "epoch:  229  loss:  0.49937111139297485\n",
      "epoch:  230  loss:  0.499223530292511\n",
      "epoch:  231  loss:  0.49907591938972473\n",
      "epoch:  232  loss:  0.4989285171031952\n",
      "epoch:  233  loss:  0.49878111481666565\n",
      "epoch:  234  loss:  0.49863380193710327\n",
      "epoch:  235  loss:  0.49848660826683044\n",
      "epoch:  236  loss:  0.49833953380584717\n",
      "epoch:  237  loss:  0.4981924891471863\n",
      "epoch:  238  loss:  0.49804550409317017\n",
      "epoch:  239  loss:  0.4978986382484436\n",
      "epoch:  240  loss:  0.4977518916130066\n",
      "epoch:  241  loss:  0.4976051449775696\n",
      "epoch:  242  loss:  0.4974585175514221\n",
      "epoch:  243  loss:  0.4973119795322418\n",
      "epoch:  244  loss:  0.4971655309200287\n",
      "epoch:  245  loss:  0.4970192015171051\n",
      "epoch:  246  loss:  0.4968729615211487\n",
      "epoch:  247  loss:  0.49672672152519226\n",
      "epoch:  248  loss:  0.4965806305408478\n",
      "epoch:  249  loss:  0.4964345693588257\n",
      "epoch:  250  loss:  0.4962886571884155\n",
      "epoch:  251  loss:  0.49614277482032776\n",
      "epoch:  252  loss:  0.49599701166152954\n",
      "epoch:  253  loss:  0.4958513081073761\n",
      "epoch:  254  loss:  0.49570566415786743\n",
      "epoch:  255  loss:  0.4955601692199707\n",
      "epoch:  256  loss:  0.49541473388671875\n",
      "epoch:  257  loss:  0.49526941776275635\n",
      "epoch:  258  loss:  0.4951241612434387\n",
      "epoch:  259  loss:  0.49497896432876587\n",
      "epoch:  260  loss:  0.4948338270187378\n",
      "epoch:  261  loss:  0.49468886852264404\n",
      "epoch:  262  loss:  0.4945439398288727\n",
      "epoch:  263  loss:  0.4943990707397461\n",
      "epoch:  264  loss:  0.49425429105758667\n",
      "epoch:  265  loss:  0.4941096007823944\n",
      "epoch:  266  loss:  0.4939649701118469\n",
      "epoch:  267  loss:  0.4938204288482666\n",
      "epoch:  268  loss:  0.49367600679397583\n",
      "epoch:  269  loss:  0.4935316741466522\n",
      "epoch:  270  loss:  0.4933874309062958\n",
      "epoch:  271  loss:  0.4932433068752289\n",
      "epoch:  272  loss:  0.4930992126464844\n",
      "epoch:  273  loss:  0.4929552674293518\n",
      "epoch:  274  loss:  0.4928113520145416\n",
      "epoch:  275  loss:  0.492667555809021\n",
      "epoch:  276  loss:  0.49252381920814514\n",
      "epoch:  277  loss:  0.49238014221191406\n",
      "epoch:  278  loss:  0.49223658442497253\n",
      "epoch:  279  loss:  0.49209311604499817\n",
      "epoch:  280  loss:  0.49194973707199097\n",
      "epoch:  281  loss:  0.4918064475059509\n",
      "epoch:  282  loss:  0.4916631877422333\n",
      "epoch:  283  loss:  0.4915199875831604\n",
      "epoch:  284  loss:  0.4913769066333771\n",
      "epoch:  285  loss:  0.4912339150905609\n",
      "epoch:  286  loss:  0.4910910129547119\n",
      "epoch:  287  loss:  0.49094823002815247\n",
      "epoch:  288  loss:  0.4908055067062378\n",
      "epoch:  289  loss:  0.4906628131866455\n",
      "epoch:  290  loss:  0.49052026867866516\n",
      "epoch:  291  loss:  0.4903777837753296\n",
      "epoch:  292  loss:  0.4902353286743164\n",
      "epoch:  293  loss:  0.4900929927825928\n",
      "epoch:  294  loss:  0.4899507462978363\n",
      "epoch:  295  loss:  0.489808589220047\n",
      "epoch:  296  loss:  0.48966652154922485\n",
      "epoch:  297  loss:  0.4895244836807251\n",
      "epoch:  298  loss:  0.4893825948238373\n",
      "epoch:  299  loss:  0.48924073576927185\n",
      "epoch:  300  loss:  0.48909902572631836\n",
      "epoch:  301  loss:  0.48895731568336487\n",
      "epoch:  302  loss:  0.48881569504737854\n",
      "epoch:  303  loss:  0.4886741638183594\n",
      "epoch:  304  loss:  0.48853275179862976\n",
      "epoch:  305  loss:  0.4883913993835449\n",
      "epoch:  306  loss:  0.48825013637542725\n",
      "epoch:  307  loss:  0.48810893297195435\n",
      "epoch:  308  loss:  0.487967848777771\n",
      "epoch:  309  loss:  0.4878268241882324\n",
      "epoch:  310  loss:  0.4876858592033386\n",
      "epoch:  311  loss:  0.487544983625412\n",
      "epoch:  312  loss:  0.4874041676521301\n",
      "epoch:  313  loss:  0.4872634708881378\n",
      "epoch:  314  loss:  0.48712289333343506\n",
      "epoch:  315  loss:  0.4869823157787323\n",
      "epoch:  316  loss:  0.4868418574333191\n",
      "epoch:  317  loss:  0.48670148849487305\n",
      "epoch:  318  loss:  0.4865611791610718\n",
      "epoch:  319  loss:  0.4864209294319153\n",
      "epoch:  320  loss:  0.48628079891204834\n",
      "epoch:  321  loss:  0.48614075779914856\n",
      "epoch:  322  loss:  0.48600074648857117\n",
      "epoch:  323  loss:  0.4858608543872833\n",
      "epoch:  324  loss:  0.48572105169296265\n",
      "epoch:  325  loss:  0.48558130860328674\n",
      "epoch:  326  loss:  0.4854416251182556\n",
      "epoch:  327  loss:  0.48530203104019165\n",
      "epoch:  328  loss:  0.48516249656677246\n",
      "epoch:  329  loss:  0.4850231409072876\n",
      "epoch:  330  loss:  0.48488375544548035\n",
      "epoch:  331  loss:  0.48474451899528503\n",
      "epoch:  332  loss:  0.4846053421497345\n",
      "epoch:  333  loss:  0.4844662547111511\n",
      "epoch:  334  loss:  0.48432719707489014\n",
      "epoch:  335  loss:  0.4841882586479187\n",
      "epoch:  336  loss:  0.48404940962791443\n",
      "epoch:  337  loss:  0.48391062021255493\n",
      "epoch:  338  loss:  0.4837718904018402\n",
      "epoch:  339  loss:  0.4836333096027374\n",
      "epoch:  340  loss:  0.48349469900131226\n",
      "epoch:  341  loss:  0.4833562672138214\n",
      "epoch:  342  loss:  0.48321783542633057\n",
      "epoch:  343  loss:  0.48307958245277405\n",
      "epoch:  344  loss:  0.48294132947921753\n",
      "epoch:  345  loss:  0.4828031659126282\n",
      "epoch:  346  loss:  0.482665091753006\n",
      "epoch:  347  loss:  0.48252710700035095\n",
      "epoch:  348  loss:  0.48238927125930786\n",
      "epoch:  349  loss:  0.4822514057159424\n",
      "epoch:  350  loss:  0.48211362957954407\n",
      "epoch:  351  loss:  0.4819759428501129\n",
      "epoch:  352  loss:  0.4818383455276489\n",
      "epoch:  353  loss:  0.4817008078098297\n",
      "epoch:  354  loss:  0.48156338930130005\n",
      "epoch:  355  loss:  0.48142606019973755\n",
      "epoch:  356  loss:  0.48128873109817505\n",
      "epoch:  357  loss:  0.4811514914035797\n",
      "epoch:  358  loss:  0.4810144305229187\n",
      "epoch:  359  loss:  0.4808773994445801\n",
      "epoch:  360  loss:  0.48074036836624146\n",
      "epoch:  361  loss:  0.48060348629951477\n",
      "epoch:  362  loss:  0.48046672344207764\n",
      "epoch:  363  loss:  0.4803299307823181\n",
      "epoch:  364  loss:  0.48019325733184814\n",
      "epoch:  365  loss:  0.4800567626953125\n",
      "epoch:  366  loss:  0.4799202084541321\n",
      "epoch:  367  loss:  0.479783833026886\n",
      "epoch:  368  loss:  0.4796474575996399\n",
      "epoch:  369  loss:  0.47951123118400574\n",
      "epoch:  370  loss:  0.4793749749660492\n",
      "epoch:  371  loss:  0.4792388677597046\n",
      "epoch:  372  loss:  0.47910287976264954\n",
      "epoch:  373  loss:  0.47896692156791687\n",
      "epoch:  374  loss:  0.47883108258247375\n",
      "epoch:  375  loss:  0.4786953032016754\n",
      "epoch:  376  loss:  0.4785595238208771\n",
      "epoch:  377  loss:  0.4784238934516907\n",
      "epoch:  378  loss:  0.47828832268714905\n",
      "epoch:  379  loss:  0.47815296053886414\n",
      "epoch:  380  loss:  0.4780177175998688\n",
      "epoch:  381  loss:  0.47788241505622864\n",
      "epoch:  382  loss:  0.4777473509311676\n",
      "epoch:  383  loss:  0.47761228680610657\n",
      "epoch:  384  loss:  0.4774772822856903\n",
      "epoch:  385  loss:  0.477342426776886\n",
      "epoch:  386  loss:  0.47720760107040405\n",
      "epoch:  387  loss:  0.4770728349685669\n",
      "epoch:  388  loss:  0.4769381284713745\n",
      "epoch:  389  loss:  0.47680357098579407\n",
      "epoch:  390  loss:  0.4766690135002136\n",
      "epoch:  391  loss:  0.47653457522392273\n",
      "epoch:  392  loss:  0.4764001965522766\n",
      "epoch:  393  loss:  0.47626590728759766\n",
      "epoch:  394  loss:  0.47613173723220825\n",
      "epoch:  395  loss:  0.47599759697914124\n",
      "epoch:  396  loss:  0.4758634865283966\n",
      "epoch:  397  loss:  0.4757295250892639\n",
      "epoch:  398  loss:  0.4755955934524536\n",
      "epoch:  399  loss:  0.47546181082725525\n",
      "epoch:  400  loss:  0.4753280580043793\n",
      "epoch:  401  loss:  0.47519436478614807\n",
      "epoch:  402  loss:  0.47506073117256165\n",
      "epoch:  403  loss:  0.47492721676826477\n",
      "epoch:  404  loss:  0.47479382157325745\n",
      "epoch:  405  loss:  0.47466039657592773\n",
      "epoch:  406  loss:  0.47452712059020996\n",
      "epoch:  407  loss:  0.4743938744068146\n",
      "epoch:  408  loss:  0.47426077723503113\n",
      "epoch:  409  loss:  0.4741276800632477\n",
      "epoch:  410  loss:  0.47399473190307617\n",
      "epoch:  411  loss:  0.47386178374290466\n",
      "epoch:  412  loss:  0.47372889518737793\n",
      "epoch:  413  loss:  0.4735961854457855\n",
      "epoch:  414  loss:  0.4734634757041931\n",
      "epoch:  415  loss:  0.47333085536956787\n",
      "epoch:  416  loss:  0.4731982350349426\n",
      "epoch:  417  loss:  0.47306567430496216\n",
      "epoch:  418  loss:  0.47293317317962646\n",
      "epoch:  419  loss:  0.47280076146125793\n",
      "epoch:  420  loss:  0.47266843914985657\n",
      "epoch:  421  loss:  0.4725361466407776\n",
      "epoch:  422  loss:  0.47240397334098816\n",
      "epoch:  423  loss:  0.4722718596458435\n",
      "epoch:  424  loss:  0.4721398651599884\n",
      "epoch:  425  loss:  0.4720078706741333\n",
      "epoch:  426  loss:  0.47187602519989014\n",
      "epoch:  427  loss:  0.47174420952796936\n",
      "epoch:  428  loss:  0.47161245346069336\n",
      "epoch:  429  loss:  0.4714807868003845\n",
      "epoch:  430  loss:  0.47134920954704285\n",
      "epoch:  431  loss:  0.47121769189834595\n",
      "epoch:  432  loss:  0.4710862636566162\n",
      "epoch:  433  loss:  0.47095492482185364\n",
      "epoch:  434  loss:  0.47082361578941345\n",
      "epoch:  435  loss:  0.4706924259662628\n",
      "epoch:  436  loss:  0.47056126594543457\n",
      "epoch:  437  loss:  0.47043025493621826\n",
      "epoch:  438  loss:  0.47029924392700195\n",
      "epoch:  439  loss:  0.4701683521270752\n",
      "epoch:  440  loss:  0.4700374901294708\n",
      "epoch:  441  loss:  0.469906747341156\n",
      "epoch:  442  loss:  0.46977606415748596\n",
      "epoch:  443  loss:  0.4696454405784607\n",
      "epoch:  444  loss:  0.4695149064064026\n",
      "epoch:  445  loss:  0.46938449144363403\n",
      "epoch:  446  loss:  0.4692540764808655\n",
      "epoch:  447  loss:  0.4691237509250641\n",
      "epoch:  448  loss:  0.46899348497390747\n",
      "epoch:  449  loss:  0.4688633680343628\n",
      "epoch:  450  loss:  0.4687332510948181\n",
      "epoch:  451  loss:  0.468603253364563\n",
      "epoch:  452  loss:  0.46847328543663025\n",
      "epoch:  453  loss:  0.46834343671798706\n",
      "epoch:  454  loss:  0.46821361780166626\n",
      "epoch:  455  loss:  0.4680838882923126\n",
      "epoch:  456  loss:  0.46795424818992615\n",
      "epoch:  457  loss:  0.46782466769218445\n",
      "epoch:  458  loss:  0.4676951766014099\n",
      "epoch:  459  loss:  0.46756574511528015\n",
      "epoch:  460  loss:  0.46743637323379517\n",
      "epoch:  461  loss:  0.46730709075927734\n",
      "epoch:  462  loss:  0.4671778976917267\n",
      "epoch:  463  loss:  0.4670487642288208\n",
      "epoch:  464  loss:  0.46691974997520447\n",
      "epoch:  465  loss:  0.46679073572158813\n",
      "epoch:  466  loss:  0.46666184067726135\n",
      "epoch:  467  loss:  0.46653297543525696\n",
      "epoch:  468  loss:  0.4664042294025421\n",
      "epoch:  469  loss:  0.46627554297447205\n",
      "epoch:  470  loss:  0.46614694595336914\n",
      "epoch:  471  loss:  0.4660183787345886\n",
      "epoch:  472  loss:  0.4658898711204529\n",
      "epoch:  473  loss:  0.4657614827156067\n",
      "epoch:  474  loss:  0.4656331539154053\n",
      "epoch:  475  loss:  0.465504914522171\n",
      "epoch:  476  loss:  0.46537673473358154\n",
      "epoch:  477  loss:  0.46524861454963684\n",
      "epoch:  478  loss:  0.4651205837726593\n",
      "epoch:  479  loss:  0.46499261260032654\n",
      "epoch:  480  loss:  0.46486473083496094\n",
      "epoch:  481  loss:  0.4647369086742401\n",
      "epoch:  482  loss:  0.46460917592048645\n",
      "epoch:  483  loss:  0.46448150277137756\n",
      "epoch:  484  loss:  0.46435388922691345\n",
      "epoch:  485  loss:  0.4642263650894165\n",
      "epoch:  486  loss:  0.4640989303588867\n",
      "epoch:  487  loss:  0.4639715254306793\n",
      "epoch:  488  loss:  0.46384429931640625\n",
      "epoch:  489  loss:  0.46371710300445557\n",
      "epoch:  490  loss:  0.46359002590179443\n",
      "epoch:  491  loss:  0.4634629786014557\n",
      "epoch:  492  loss:  0.4633360505104065\n",
      "epoch:  493  loss:  0.4632091522216797\n",
      "epoch:  494  loss:  0.46308234333992004\n",
      "epoch:  495  loss:  0.4629555940628052\n",
      "epoch:  496  loss:  0.46282893419265747\n",
      "epoch:  497  loss:  0.46270236372947693\n",
      "epoch:  498  loss:  0.46257588267326355\n",
      "epoch:  499  loss:  0.46244943141937256\n",
      "epoch:  500  loss:  0.46232306957244873\n",
      "epoch:  501  loss:  0.4621967673301697\n",
      "epoch:  502  loss:  0.4620705246925354\n",
      "epoch:  503  loss:  0.4619443714618683\n",
      "epoch:  504  loss:  0.46181824803352356\n",
      "epoch:  505  loss:  0.4616922438144684\n",
      "epoch:  506  loss:  0.461566299200058\n",
      "epoch:  507  loss:  0.46144044399261475\n",
      "epoch:  508  loss:  0.4613146185874939\n",
      "epoch:  509  loss:  0.4611889123916626\n",
      "epoch:  510  loss:  0.4610632658004761\n",
      "epoch:  511  loss:  0.4609376788139343\n",
      "epoch:  512  loss:  0.46081218123435974\n",
      "epoch:  513  loss:  0.46068668365478516\n",
      "epoch:  514  loss:  0.4605613052845001\n",
      "epoch:  515  loss:  0.460436075925827\n",
      "epoch:  516  loss:  0.46031084656715393\n",
      "epoch:  517  loss:  0.4601856470108032\n",
      "epoch:  518  loss:  0.46006059646606445\n",
      "epoch:  519  loss:  0.45993560552597046\n",
      "epoch:  520  loss:  0.4598105847835541\n",
      "epoch:  521  loss:  0.45968571305274963\n",
      "epoch:  522  loss:  0.45956096053123474\n",
      "epoch:  523  loss:  0.45943617820739746\n",
      "epoch:  524  loss:  0.45931151509284973\n",
      "epoch:  525  loss:  0.45918694138526917\n",
      "epoch:  526  loss:  0.459062397480011\n",
      "epoch:  527  loss:  0.45893797278404236\n",
      "epoch:  528  loss:  0.4588135778903961\n",
      "epoch:  529  loss:  0.45868927240371704\n",
      "epoch:  530  loss:  0.45856502652168274\n",
      "epoch:  531  loss:  0.4584408700466156\n",
      "epoch:  532  loss:  0.45831677317619324\n",
      "epoch:  533  loss:  0.45819270610809326\n",
      "epoch:  534  loss:  0.4580687880516052\n",
      "epoch:  535  loss:  0.4579448997974396\n",
      "epoch:  536  loss:  0.4578211009502411\n",
      "epoch:  537  loss:  0.457697331905365\n",
      "epoch:  538  loss:  0.45757368206977844\n",
      "epoch:  539  loss:  0.4574500620365143\n",
      "epoch:  540  loss:  0.4573264718055725\n",
      "epoch:  541  loss:  0.45720306038856506\n",
      "epoch:  542  loss:  0.4570796489715576\n",
      "epoch:  543  loss:  0.45695632696151733\n",
      "epoch:  544  loss:  0.4568330943584442\n",
      "epoch:  545  loss:  0.4567098915576935\n",
      "epoch:  546  loss:  0.4565868079662323\n",
      "epoch:  547  loss:  0.4564637243747711\n",
      "epoch:  548  loss:  0.4563407897949219\n",
      "epoch:  549  loss:  0.45621782541275024\n",
      "epoch:  550  loss:  0.45609503984451294\n",
      "epoch:  551  loss:  0.45597222447395325\n",
      "epoch:  552  loss:  0.4558494985103607\n",
      "epoch:  553  loss:  0.4557269215583801\n",
      "epoch:  554  loss:  0.45560434460639954\n",
      "epoch:  555  loss:  0.4554818570613861\n",
      "epoch:  556  loss:  0.45535948872566223\n",
      "epoch:  557  loss:  0.45523709058761597\n",
      "epoch:  558  loss:  0.45511478185653687\n",
      "epoch:  559  loss:  0.4549926221370697\n",
      "epoch:  560  loss:  0.45487046241760254\n",
      "epoch:  561  loss:  0.4547484219074249\n",
      "epoch:  562  loss:  0.45462632179260254\n",
      "epoch:  563  loss:  0.45450443029403687\n",
      "epoch:  564  loss:  0.4543825685977936\n",
      "epoch:  565  loss:  0.4542607367038727\n",
      "epoch:  566  loss:  0.4541390538215637\n",
      "epoch:  567  loss:  0.45401740074157715\n",
      "epoch:  568  loss:  0.45389580726623535\n",
      "epoch:  569  loss:  0.45377427339553833\n",
      "epoch:  570  loss:  0.45365282893180847\n",
      "epoch:  571  loss:  0.4535314738750458\n",
      "epoch:  572  loss:  0.45341014862060547\n",
      "epoch:  573  loss:  0.45328888297080994\n",
      "epoch:  574  loss:  0.4531676769256592\n",
      "epoch:  575  loss:  0.45304661989212036\n",
      "epoch:  576  loss:  0.45292559266090393\n",
      "epoch:  577  loss:  0.4528045952320099\n",
      "epoch:  578  loss:  0.452683687210083\n",
      "epoch:  579  loss:  0.4525628685951233\n",
      "epoch:  580  loss:  0.4524420499801636\n",
      "epoch:  581  loss:  0.4523214101791382\n",
      "epoch:  582  loss:  0.4522008001804352\n",
      "epoch:  583  loss:  0.45208021998405457\n",
      "epoch:  584  loss:  0.4519597291946411\n",
      "epoch:  585  loss:  0.45183926820755005\n",
      "epoch:  586  loss:  0.45171892642974854\n",
      "epoch:  587  loss:  0.4515986144542694\n",
      "epoch:  588  loss:  0.45147842168807983\n",
      "epoch:  589  loss:  0.45135828852653503\n",
      "epoch:  590  loss:  0.451238214969635\n",
      "epoch:  591  loss:  0.45111820101737976\n",
      "epoch:  592  loss:  0.4509982466697693\n",
      "epoch:  593  loss:  0.450878381729126\n",
      "epoch:  594  loss:  0.45075854659080505\n",
      "epoch:  595  loss:  0.4506388306617737\n",
      "epoch:  596  loss:  0.4505191445350647\n",
      "epoch:  597  loss:  0.4503995180130005\n",
      "epoch:  598  loss:  0.45027998089790344\n",
      "epoch:  599  loss:  0.45016050338745117\n",
      "epoch:  600  loss:  0.45004111528396606\n",
      "epoch:  601  loss:  0.44992172718048096\n",
      "epoch:  602  loss:  0.44980254769325256\n",
      "epoch:  603  loss:  0.4496833086013794\n",
      "epoch:  604  loss:  0.4495641589164734\n",
      "epoch:  605  loss:  0.44944506883621216\n",
      "epoch:  606  loss:  0.4493260979652405\n",
      "epoch:  607  loss:  0.4492071568965912\n",
      "epoch:  608  loss:  0.44908830523490906\n",
      "epoch:  609  loss:  0.4489695131778717\n",
      "epoch:  610  loss:  0.44885075092315674\n",
      "epoch:  611  loss:  0.44873207807540894\n",
      "epoch:  612  loss:  0.4486134946346283\n",
      "epoch:  613  loss:  0.4484950006008148\n",
      "epoch:  614  loss:  0.44837653636932373\n",
      "epoch:  615  loss:  0.44825810194015503\n",
      "epoch:  616  loss:  0.44813981652259827\n",
      "epoch:  617  loss:  0.4480215013027191\n",
      "epoch:  618  loss:  0.4479033946990967\n",
      "epoch:  619  loss:  0.4477851986885071\n",
      "epoch:  620  loss:  0.4476671814918518\n",
      "epoch:  621  loss:  0.44754913449287415\n",
      "epoch:  622  loss:  0.4474312365055084\n",
      "epoch:  623  loss:  0.4473133683204651\n",
      "epoch:  624  loss:  0.4471955895423889\n",
      "epoch:  625  loss:  0.44707784056663513\n",
      "epoch:  626  loss:  0.4469601809978485\n",
      "epoch:  627  loss:  0.44684261083602905\n",
      "epoch:  628  loss:  0.446725070476532\n",
      "epoch:  629  loss:  0.4466075897216797\n",
      "epoch:  630  loss:  0.44649019837379456\n",
      "epoch:  631  loss:  0.4463728666305542\n",
      "epoch:  632  loss:  0.4462555944919586\n",
      "epoch:  633  loss:  0.4461384117603302\n",
      "epoch:  634  loss:  0.44602125883102417\n",
      "epoch:  635  loss:  0.4459042251110077\n",
      "epoch:  636  loss:  0.4457872211933136\n",
      "epoch:  637  loss:  0.44567030668258667\n",
      "epoch:  638  loss:  0.44555342197418213\n",
      "epoch:  639  loss:  0.44543662667274475\n",
      "epoch:  640  loss:  0.44531989097595215\n",
      "epoch:  641  loss:  0.4452032446861267\n",
      "epoch:  642  loss:  0.4450865685939789\n",
      "epoch:  643  loss:  0.4449700713157654\n",
      "epoch:  644  loss:  0.44485369324684143\n",
      "epoch:  645  loss:  0.44473737478256226\n",
      "epoch:  646  loss:  0.44462111592292786\n",
      "epoch:  647  loss:  0.44450491666793823\n",
      "epoch:  648  loss:  0.44438883662223816\n",
      "epoch:  649  loss:  0.44427284598350525\n",
      "epoch:  650  loss:  0.44415685534477234\n",
      "epoch:  651  loss:  0.4440409541130066\n",
      "epoch:  652  loss:  0.443925142288208\n",
      "epoch:  653  loss:  0.4438093304634094\n",
      "epoch:  654  loss:  0.4436936378479004\n",
      "epoch:  655  loss:  0.44357800483703613\n",
      "epoch:  656  loss:  0.4434625506401062\n",
      "epoch:  657  loss:  0.44334709644317627\n",
      "epoch:  658  loss:  0.4432317614555359\n",
      "epoch:  659  loss:  0.4431164860725403\n",
      "epoch:  660  loss:  0.4430011808872223\n",
      "epoch:  661  loss:  0.4428860545158386\n",
      "epoch:  662  loss:  0.44277098774909973\n",
      "epoch:  663  loss:  0.44265592098236084\n",
      "epoch:  664  loss:  0.4425409734249115\n",
      "epoch:  665  loss:  0.44242605566978455\n",
      "epoch:  666  loss:  0.44231116771698\n",
      "epoch:  667  loss:  0.44219642877578735\n",
      "epoch:  668  loss:  0.4420817792415619\n",
      "epoch:  669  loss:  0.4419671297073364\n",
      "epoch:  670  loss:  0.44185253977775574\n",
      "epoch:  671  loss:  0.4417380690574646\n",
      "epoch:  672  loss:  0.4416235685348511\n",
      "epoch:  673  loss:  0.4415091574192047\n",
      "epoch:  674  loss:  0.4413948357105255\n",
      "epoch:  675  loss:  0.4412805438041687\n",
      "epoch:  676  loss:  0.44116640090942383\n",
      "epoch:  677  loss:  0.44105225801467896\n",
      "epoch:  678  loss:  0.44093817472457886\n",
      "epoch:  679  loss:  0.44082415103912354\n",
      "epoch:  680  loss:  0.440710186958313\n",
      "epoch:  681  loss:  0.440596342086792\n",
      "epoch:  682  loss:  0.44048258662223816\n",
      "epoch:  683  loss:  0.4403688311576843\n",
      "epoch:  684  loss:  0.44025516510009766\n",
      "epoch:  685  loss:  0.4401415288448334\n",
      "epoch:  686  loss:  0.44002798199653625\n",
      "epoch:  687  loss:  0.4399144649505615\n",
      "epoch:  688  loss:  0.43980109691619873\n",
      "epoch:  689  loss:  0.43968769907951355\n",
      "epoch:  690  loss:  0.43957439064979553\n",
      "epoch:  691  loss:  0.43946123123168945\n",
      "epoch:  692  loss:  0.4393479824066162\n",
      "epoch:  693  loss:  0.4392349123954773\n",
      "epoch:  694  loss:  0.4391218423843384\n",
      "epoch:  695  loss:  0.439008891582489\n",
      "epoch:  696  loss:  0.43889594078063965\n",
      "epoch:  697  loss:  0.43878310918807983\n",
      "epoch:  698  loss:  0.4386703372001648\n",
      "epoch:  699  loss:  0.43855762481689453\n",
      "epoch:  700  loss:  0.43844497203826904\n",
      "epoch:  701  loss:  0.43833231925964355\n",
      "epoch:  702  loss:  0.43821981549263\n",
      "epoch:  703  loss:  0.43810734152793884\n",
      "epoch:  704  loss:  0.43799492716789246\n",
      "epoch:  705  loss:  0.43788260221481323\n",
      "epoch:  706  loss:  0.437770277261734\n",
      "epoch:  707  loss:  0.43765804171562195\n",
      "epoch:  708  loss:  0.43754592537879944\n",
      "epoch:  709  loss:  0.43743380904197693\n",
      "epoch:  710  loss:  0.4373217821121216\n",
      "epoch:  711  loss:  0.43720972537994385\n",
      "epoch:  712  loss:  0.4370976984500885\n",
      "epoch:  713  loss:  0.4369857609272003\n",
      "epoch:  714  loss:  0.4368739128112793\n",
      "epoch:  715  loss:  0.43676209449768066\n",
      "epoch:  716  loss:  0.4366503059864044\n",
      "epoch:  717  loss:  0.4365386664867401\n",
      "epoch:  718  loss:  0.4364270269870758\n",
      "epoch:  719  loss:  0.43631547689437866\n",
      "epoch:  720  loss:  0.4362039864063263\n",
      "epoch:  721  loss:  0.4360924959182739\n",
      "epoch:  722  loss:  0.43598121404647827\n",
      "epoch:  723  loss:  0.43586987257003784\n",
      "epoch:  724  loss:  0.43575865030288696\n",
      "epoch:  725  loss:  0.43564748764038086\n",
      "epoch:  726  loss:  0.43553638458251953\n",
      "epoch:  727  loss:  0.4354252815246582\n",
      "epoch:  728  loss:  0.4353142976760864\n",
      "epoch:  729  loss:  0.43520334362983704\n",
      "epoch:  730  loss:  0.4350924789905548\n",
      "epoch:  731  loss:  0.43498167395591736\n",
      "epoch:  732  loss:  0.43487095832824707\n",
      "epoch:  733  loss:  0.4347602128982544\n",
      "epoch:  734  loss:  0.43464961647987366\n",
      "epoch:  735  loss:  0.4345390796661377\n",
      "epoch:  736  loss:  0.43442854285240173\n",
      "epoch:  737  loss:  0.4343181252479553\n",
      "epoch:  738  loss:  0.4342077672481537\n",
      "epoch:  739  loss:  0.43409740924835205\n",
      "epoch:  740  loss:  0.43398720026016235\n",
      "epoch:  741  loss:  0.43387705087661743\n",
      "epoch:  742  loss:  0.4337668716907501\n",
      "epoch:  743  loss:  0.43365687131881714\n",
      "epoch:  744  loss:  0.43354684114456177\n",
      "epoch:  745  loss:  0.43343687057495117\n",
      "epoch:  746  loss:  0.4333270192146301\n",
      "epoch:  747  loss:  0.43321719765663147\n",
      "epoch:  748  loss:  0.4331074059009552\n",
      "epoch:  749  loss:  0.43299776315689087\n",
      "epoch:  750  loss:  0.43288809061050415\n",
      "epoch:  751  loss:  0.43277856707572937\n",
      "epoch:  752  loss:  0.432669073343277\n",
      "epoch:  753  loss:  0.43255963921546936\n",
      "epoch:  754  loss:  0.43245020508766174\n",
      "epoch:  755  loss:  0.43234091997146606\n",
      "epoch:  756  loss:  0.4322316646575928\n",
      "epoch:  757  loss:  0.43212243914604187\n",
      "epoch:  758  loss:  0.4320133328437805\n",
      "epoch:  759  loss:  0.43190425634384155\n",
      "epoch:  760  loss:  0.431795209646225\n",
      "epoch:  761  loss:  0.43168625235557556\n",
      "epoch:  762  loss:  0.4315773844718933\n",
      "epoch:  763  loss:  0.43146854639053345\n",
      "epoch:  764  loss:  0.43135976791381836\n",
      "epoch:  765  loss:  0.43125101923942566\n",
      "epoch:  766  loss:  0.4311424493789673\n",
      "epoch:  767  loss:  0.4310338497161865\n",
      "epoch:  768  loss:  0.43092530965805054\n",
      "epoch:  769  loss:  0.4308168292045593\n",
      "epoch:  770  loss:  0.4307084381580353\n",
      "epoch:  771  loss:  0.430600106716156\n",
      "epoch:  772  loss:  0.4304918050765991\n",
      "epoch:  773  loss:  0.4303835928440094\n",
      "epoch:  774  loss:  0.43027544021606445\n",
      "epoch:  775  loss:  0.4301673471927643\n",
      "epoch:  776  loss:  0.4300592839717865\n",
      "epoch:  777  loss:  0.4299513101577759\n",
      "epoch:  778  loss:  0.4298434257507324\n",
      "epoch:  779  loss:  0.4297355115413666\n",
      "epoch:  780  loss:  0.42962774634361267\n",
      "epoch:  781  loss:  0.42951998114585876\n",
      "epoch:  782  loss:  0.4294123351573944\n",
      "epoch:  783  loss:  0.42930468916893005\n",
      "epoch:  784  loss:  0.42919716238975525\n",
      "epoch:  785  loss:  0.42908966541290283\n",
      "epoch:  786  loss:  0.42898228764533997\n",
      "epoch:  787  loss:  0.42887482047080994\n",
      "epoch:  788  loss:  0.4287675619125366\n",
      "epoch:  789  loss:  0.4286602735519409\n",
      "epoch:  790  loss:  0.42855310440063477\n",
      "epoch:  791  loss:  0.4284459948539734\n",
      "epoch:  792  loss:  0.4283389151096344\n",
      "epoch:  793  loss:  0.4282318949699402\n",
      "epoch:  794  loss:  0.42812493443489075\n",
      "epoch:  795  loss:  0.4280180037021637\n",
      "epoch:  796  loss:  0.4279112219810486\n",
      "epoch:  797  loss:  0.42780444025993347\n",
      "epoch:  798  loss:  0.4276977479457855\n",
      "epoch:  799  loss:  0.42759108543395996\n",
      "epoch:  800  loss:  0.4274844527244568\n",
      "epoch:  801  loss:  0.42737796902656555\n",
      "epoch:  802  loss:  0.4272714853286743\n",
      "epoch:  803  loss:  0.4271650016307831\n",
      "epoch:  804  loss:  0.42705869674682617\n",
      "epoch:  805  loss:  0.4269523620605469\n",
      "epoch:  806  loss:  0.42684614658355713\n",
      "epoch:  807  loss:  0.42673999071121216\n",
      "epoch:  808  loss:  0.4266338646411896\n",
      "epoch:  809  loss:  0.42652779817581177\n",
      "epoch:  810  loss:  0.4264218509197235\n",
      "epoch:  811  loss:  0.42631587386131287\n",
      "epoch:  812  loss:  0.4262099862098694\n",
      "epoch:  813  loss:  0.4261041581630707\n",
      "epoch:  814  loss:  0.42599838972091675\n",
      "epoch:  815  loss:  0.42589271068573\n",
      "epoch:  816  loss:  0.4257870614528656\n",
      "epoch:  817  loss:  0.4256815016269684\n",
      "epoch:  818  loss:  0.42557594180107117\n",
      "epoch:  819  loss:  0.4254705011844635\n",
      "epoch:  820  loss:  0.4253650903701782\n",
      "epoch:  821  loss:  0.4252597689628601\n",
      "epoch:  822  loss:  0.425154447555542\n",
      "epoch:  823  loss:  0.42504921555519104\n",
      "epoch:  824  loss:  0.42494407296180725\n",
      "epoch:  825  loss:  0.42483896017074585\n",
      "epoch:  826  loss:  0.4247339069843292\n",
      "epoch:  827  loss:  0.42462897300720215\n",
      "epoch:  828  loss:  0.4245240092277527\n",
      "epoch:  829  loss:  0.4244191348552704\n",
      "epoch:  830  loss:  0.42431434988975525\n",
      "epoch:  831  loss:  0.4242095649242401\n",
      "epoch:  832  loss:  0.42410486936569214\n",
      "epoch:  833  loss:  0.42400023341178894\n",
      "epoch:  834  loss:  0.4238956570625305\n",
      "epoch:  835  loss:  0.42379114031791687\n",
      "epoch:  836  loss:  0.4236866235733032\n",
      "epoch:  837  loss:  0.4235822856426239\n",
      "epoch:  838  loss:  0.4234779477119446\n",
      "epoch:  839  loss:  0.42337363958358765\n",
      "epoch:  840  loss:  0.4232693612575531\n",
      "epoch:  841  loss:  0.4231652319431305\n",
      "epoch:  842  loss:  0.4230610728263855\n",
      "epoch:  843  loss:  0.42295703291893005\n",
      "epoch:  844  loss:  0.4228530824184418\n",
      "epoch:  845  loss:  0.4227491319179535\n",
      "epoch:  846  loss:  0.4226452708244324\n",
      "epoch:  847  loss:  0.42254143953323364\n",
      "epoch:  848  loss:  0.4224376678466797\n",
      "epoch:  849  loss:  0.4223339557647705\n",
      "epoch:  850  loss:  0.4222303032875061\n",
      "epoch:  851  loss:  0.4221267104148865\n",
      "epoch:  852  loss:  0.422023206949234\n",
      "epoch:  853  loss:  0.42191973328590393\n",
      "epoch:  854  loss:  0.42181631922721863\n",
      "epoch:  855  loss:  0.4217129647731781\n",
      "epoch:  856  loss:  0.42160969972610474\n",
      "epoch:  857  loss:  0.42150643467903137\n",
      "epoch:  858  loss:  0.4214032292366028\n",
      "epoch:  859  loss:  0.42130011320114136\n",
      "epoch:  860  loss:  0.42119699716567993\n",
      "epoch:  861  loss:  0.42109403014183044\n",
      "epoch:  862  loss:  0.42099106311798096\n",
      "epoch:  863  loss:  0.42088818550109863\n",
      "epoch:  864  loss:  0.4207853376865387\n",
      "epoch:  865  loss:  0.4206826090812683\n",
      "epoch:  866  loss:  0.4205798804759979\n",
      "epoch:  867  loss:  0.4204771816730499\n",
      "epoch:  868  loss:  0.4203745722770691\n",
      "epoch:  869  loss:  0.4202720522880554\n",
      "epoch:  870  loss:  0.42016953229904175\n",
      "epoch:  871  loss:  0.4200671315193176\n",
      "epoch:  872  loss:  0.4199647307395935\n",
      "epoch:  873  loss:  0.41986244916915894\n",
      "epoch:  874  loss:  0.41976016759872437\n",
      "epoch:  875  loss:  0.41965797543525696\n",
      "epoch:  876  loss:  0.41955581307411194\n",
      "epoch:  877  loss:  0.4194537401199341\n",
      "epoch:  878  loss:  0.4193516969680786\n",
      "epoch:  879  loss:  0.4192497134208679\n",
      "epoch:  880  loss:  0.4191478192806244\n",
      "epoch:  881  loss:  0.41904592514038086\n",
      "epoch:  882  loss:  0.4189440906047821\n",
      "epoch:  883  loss:  0.4188424050807953\n",
      "epoch:  884  loss:  0.4187406599521637\n",
      "epoch:  885  loss:  0.41863906383514404\n",
      "epoch:  886  loss:  0.4185374677181244\n",
      "epoch:  887  loss:  0.4184359312057495\n",
      "epoch:  888  loss:  0.418334424495697\n",
      "epoch:  889  loss:  0.4182330369949341\n",
      "epoch:  890  loss:  0.41813164949417114\n",
      "epoch:  891  loss:  0.418030321598053\n",
      "epoch:  892  loss:  0.417929083108902\n",
      "epoch:  893  loss:  0.41782787442207336\n",
      "epoch:  894  loss:  0.4177267253398895\n",
      "epoch:  895  loss:  0.41762566566467285\n",
      "epoch:  896  loss:  0.41752463579177856\n",
      "epoch:  897  loss:  0.41742366552352905\n",
      "epoch:  898  loss:  0.4173227548599243\n",
      "epoch:  899  loss:  0.41722190380096436\n",
      "epoch:  900  loss:  0.41712111234664917\n",
      "epoch:  901  loss:  0.41702035069465637\n",
      "epoch:  902  loss:  0.41691964864730835\n",
      "epoch:  903  loss:  0.4168190658092499\n",
      "epoch:  904  loss:  0.4167184829711914\n",
      "epoch:  905  loss:  0.41661790013313293\n",
      "epoch:  906  loss:  0.4165174961090088\n",
      "epoch:  907  loss:  0.41641712188720703\n",
      "epoch:  908  loss:  0.4163167476654053\n",
      "epoch:  909  loss:  0.4162164330482483\n",
      "epoch:  910  loss:  0.4161161780357361\n",
      "epoch:  911  loss:  0.41601598262786865\n",
      "epoch:  912  loss:  0.4159158766269684\n",
      "epoch:  913  loss:  0.4158158302307129\n",
      "epoch:  914  loss:  0.4157158434391022\n",
      "epoch:  915  loss:  0.41561585664749146\n",
      "epoch:  916  loss:  0.4155159592628479\n",
      "epoch:  917  loss:  0.4154161214828491\n",
      "epoch:  918  loss:  0.41531631350517273\n",
      "epoch:  919  loss:  0.4152165353298187\n",
      "epoch:  920  loss:  0.41511690616607666\n",
      "epoch:  921  loss:  0.4150172770023346\n",
      "epoch:  922  loss:  0.4149177074432373\n",
      "epoch:  923  loss:  0.4148181974887848\n",
      "epoch:  924  loss:  0.41471871733665466\n",
      "epoch:  925  loss:  0.4146193563938141\n",
      "epoch:  926  loss:  0.4145199954509735\n",
      "epoch:  927  loss:  0.4144206941127777\n",
      "epoch:  928  loss:  0.4143214821815491\n",
      "epoch:  929  loss:  0.4142223000526428\n",
      "epoch:  930  loss:  0.41412317752838135\n",
      "epoch:  931  loss:  0.41402408480644226\n",
      "epoch:  932  loss:  0.41392505168914795\n",
      "epoch:  933  loss:  0.4138261079788208\n",
      "epoch:  934  loss:  0.4137272238731384\n",
      "epoch:  935  loss:  0.41362839937210083\n",
      "epoch:  936  loss:  0.41352957487106323\n",
      "epoch:  937  loss:  0.4134308397769928\n",
      "epoch:  938  loss:  0.4133321940898895\n",
      "epoch:  939  loss:  0.41323354840278625\n",
      "epoch:  940  loss:  0.41313499212265015\n",
      "epoch:  941  loss:  0.4130364954471588\n",
      "epoch:  942  loss:  0.4129379987716675\n",
      "epoch:  943  loss:  0.4128396213054657\n",
      "epoch:  944  loss:  0.4127412438392639\n",
      "epoch:  945  loss:  0.4126429557800293\n",
      "epoch:  946  loss:  0.41254472732543945\n",
      "epoch:  947  loss:  0.4124465584754944\n",
      "epoch:  948  loss:  0.4123483896255493\n",
      "epoch:  949  loss:  0.4122503399848938\n",
      "epoch:  950  loss:  0.4121522903442383\n",
      "epoch:  951  loss:  0.4120543301105499\n",
      "epoch:  952  loss:  0.41195639967918396\n",
      "epoch:  953  loss:  0.41185855865478516\n",
      "epoch:  954  loss:  0.41176077723503113\n",
      "epoch:  955  loss:  0.4116630256175995\n",
      "epoch:  956  loss:  0.4115653336048126\n",
      "epoch:  957  loss:  0.41146770119667053\n",
      "epoch:  958  loss:  0.41137009859085083\n",
      "epoch:  959  loss:  0.4112725257873535\n",
      "epoch:  960  loss:  0.41117510199546814\n",
      "epoch:  961  loss:  0.41107767820358276\n",
      "epoch:  962  loss:  0.41098031401634216\n",
      "epoch:  963  loss:  0.41088297963142395\n",
      "epoch:  964  loss:  0.4107857644557953\n",
      "epoch:  965  loss:  0.41068851947784424\n",
      "epoch:  966  loss:  0.41059136390686035\n",
      "epoch:  967  loss:  0.41049423813819885\n",
      "epoch:  968  loss:  0.4103972613811493\n",
      "epoch:  969  loss:  0.41030025482177734\n",
      "epoch:  970  loss:  0.41020333766937256\n",
      "epoch:  971  loss:  0.41010645031929016\n",
      "epoch:  972  loss:  0.41000962257385254\n",
      "epoch:  973  loss:  0.4099128842353821\n",
      "epoch:  974  loss:  0.4098161458969116\n",
      "epoch:  975  loss:  0.4097194969654083\n",
      "epoch:  976  loss:  0.40962284803390503\n",
      "epoch:  977  loss:  0.40952634811401367\n",
      "epoch:  978  loss:  0.4094298183917999\n",
      "epoch:  979  loss:  0.40933334827423096\n",
      "epoch:  980  loss:  0.40923696756362915\n",
      "epoch:  981  loss:  0.4091406464576721\n",
      "epoch:  982  loss:  0.4090443551540375\n",
      "epoch:  983  loss:  0.4089481234550476\n",
      "epoch:  984  loss:  0.4088519215583801\n",
      "epoch:  985  loss:  0.4087558090686798\n",
      "epoch:  986  loss:  0.40865975618362427\n",
      "epoch:  987  loss:  0.4085637331008911\n",
      "epoch:  988  loss:  0.40846776962280273\n",
      "epoch:  989  loss:  0.40837186574935913\n",
      "epoch:  990  loss:  0.4082759916782379\n",
      "epoch:  991  loss:  0.4081801772117615\n",
      "epoch:  992  loss:  0.4080844223499298\n",
      "epoch:  993  loss:  0.4079887568950653\n",
      "epoch:  994  loss:  0.4078930616378784\n",
      "epoch:  995  loss:  0.4077974855899811\n",
      "epoch:  996  loss:  0.4077019691467285\n",
      "epoch:  997  loss:  0.4076065123081207\n",
      "epoch:  998  loss:  0.4075110852718353\n",
      "epoch:  999  loss:  0.4074156880378723\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    y_pred_3 = model_lr_1(X)\n",
    "    loss = metrics_name_2(y_pred_3, y)\n",
    "    print('epoch: ', epoch,' loss: ', loss.item())\n",
    "    Optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    Optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4f6f01a9-a2b3-4643-85a7-fd4d4af41153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "2c28cb94-d959-4301-8048-c88c1b2a65a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_1(model_lr_1, loss_fn, optimizer):\n",
    "    \"\"\"function_1 train_1\"\"\"\n",
    "    size = len(X_train_tensor)\n",
    "    model_lr_1.train()\n",
    "    for batch_size_lr_1 in range(1000):\n",
    "        y_pred_4 = model_lr_1(X)\n",
    "        loss = loss_fn(y_pred_4, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if BATCH_SIZE_LR_1 % 16 == 0:\n",
    "            loss_3 = loss.item(), BATCH_SIZE_LR * len(X)\n",
    "            print(f'loss: {loss_3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "810dbf36-40b9-48df-a2fa-65df58b2e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"function_1 train_1\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "3cef3fa1-42b8-4ed7-95c2-fd0e4f0c3877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_1(model_lr_1, loss_fn):\n",
    "    \"\"\"function_1 test_1\"\"\"\n",
    "    size = len(X_train_tensor)\n",
    "    num_batches = size\n",
    "    model_lr_1.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_size_lr_1 in range(1000):\n",
    "            y_pred_5 = model_lr_1(X)\n",
    "            test_loss += loss_fn(y_pred_5, y).item()\n",
    "            test_loss /= num_batches\n",
    "            print(f'Test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "47a5abd2-af3b-46df-9523-2f25fd3fd7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"function_1 test_1\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6c577f3c-3dff-4e60-9d8c-8e1866f0d234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2081]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model_lr_1(X_test_tensor[-1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "de63f806-3ab3-4a19-8bf9-5660ce0d2847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_tensor[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "47ad283a-fce5-4937-820b-f994b7daac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_lr_1, \"lr_1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "92745c76-4775-4b0b-83fc-ed96ff771f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model_lr_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab86e1f-c527-460c-843d-1d7ff720c091",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
